{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader  \n",
    "\n",
    "# Check that MPS is available\n",
    "\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\n",
    "            \"MPS not available because the current PyTorch install was not \"\n",
    "            \"built with MPS enabled.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "            \"and/or you do not have an MPS-enabled device on this machine.\"\n",
    "        )\n",
    "    mps_device = None\n",
    "else:\n",
    "    mps_device = torch.device(\"mps\")\n",
    "\n",
    "if mps_device is not None:\n",
    "    device = mps_device\n",
    "    print(\"Using MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 8.4226\n",
      "Epoch 1, Batch 200, Loss: 2.5619\n",
      "Epoch 1, Batch 300, Loss: 2.4319\n",
      "Epoch 1, Batch 400, Loss: 2.2481\n",
      "Epoch 1, Batch 500, Loss: 2.2215\n",
      "Epoch 1, Batch 600, Loss: 2.2007\n",
      "Epoch 1, Batch 700, Loss: 2.1630\n",
      "Epoch 1, Batch 800, Loss: 2.1197\n",
      "Epoch 1, Batch 900, Loss: 2.0320\n",
      "Epoch 1, Batch 1000, Loss: 2.0617\n",
      "Epoch 1, Batch 1100, Loss: 2.0080\n",
      "Epoch 1, Batch 1200, Loss: 1.9134\n",
      "Epoch 1, Batch 1300, Loss: 1.9481\n",
      "Epoch 1, Batch 1400, Loss: 1.9821\n",
      "Epoch 1, Batch 1500, Loss: 1.9374\n",
      "Epoch 1, Batch 1600, Loss: 1.9530\n",
      "Epoch 1, Batch 1700, Loss: 1.9101\n",
      "Epoch 1, Batch 1800, Loss: 1.8731\n",
      "Epoch 1, Batch 1900, Loss: 1.8989\n",
      "Epoch 1, Batch 2000, Loss: 1.8434\n",
      "Epoch 1, Batch 2100, Loss: 1.7950\n",
      "Epoch 1, Batch 2200, Loss: 1.8067\n",
      "Epoch 1, Batch 2300, Loss: 1.7421\n",
      "Epoch 1, Batch 2400, Loss: 1.7764\n",
      "Epoch 1, Batch 2500, Loss: 1.7427\n",
      "Epoch 1, Batch 2600, Loss: 1.7966\n",
      "Epoch 1, Batch 2700, Loss: 1.7457\n",
      "Epoch 1, Batch 2800, Loss: 1.6834\n",
      "Epoch 1, Batch 2900, Loss: 1.6827\n",
      "Epoch 2, Batch 100, Loss: 1.7180\n",
      "Epoch 2, Batch 200, Loss: 1.6445\n",
      "Epoch 2, Batch 300, Loss: 1.6576\n",
      "Epoch 2, Batch 400, Loss: 1.6473\n",
      "Epoch 2, Batch 500, Loss: 1.6865\n",
      "Epoch 2, Batch 600, Loss: 1.6471\n",
      "Epoch 2, Batch 700, Loss: 1.6543\n",
      "Epoch 2, Batch 800, Loss: 1.6264\n",
      "Epoch 2, Batch 900, Loss: 1.6136\n",
      "Epoch 2, Batch 1000, Loss: 1.6157\n",
      "Epoch 2, Batch 1100, Loss: 1.6595\n",
      "Epoch 2, Batch 1200, Loss: 1.5934\n",
      "Epoch 2, Batch 1300, Loss: 1.5919\n",
      "Epoch 2, Batch 1400, Loss: 1.6419\n",
      "Epoch 2, Batch 1500, Loss: 1.5822\n",
      "Epoch 2, Batch 1600, Loss: 1.5590\n",
      "Epoch 2, Batch 1700, Loss: 1.6122\n",
      "Epoch 2, Batch 1800, Loss: 1.5431\n",
      "Epoch 2, Batch 1900, Loss: 1.5476\n",
      "Epoch 2, Batch 2000, Loss: 1.5327\n",
      "Epoch 2, Batch 2100, Loss: 1.5830\n",
      "Epoch 2, Batch 2200, Loss: 1.5761\n",
      "Epoch 2, Batch 2300, Loss: 1.5422\n",
      "Epoch 2, Batch 2400, Loss: 1.5302\n",
      "Epoch 2, Batch 2500, Loss: 1.5709\n",
      "Epoch 2, Batch 2600, Loss: 1.5467\n",
      "Epoch 2, Batch 2700, Loss: 1.5069\n",
      "Epoch 2, Batch 2800, Loss: 1.5176\n",
      "Epoch 2, Batch 2900, Loss: 1.4530\n",
      "Epoch 3, Batch 100, Loss: 1.4710\n",
      "Epoch 3, Batch 200, Loss: 1.4692\n",
      "Epoch 3, Batch 300, Loss: 1.4706\n",
      "Epoch 3, Batch 400, Loss: 1.4830\n",
      "Epoch 3, Batch 500, Loss: 1.4762\n",
      "Epoch 3, Batch 600, Loss: 1.4292\n",
      "Epoch 3, Batch 700, Loss: 1.4487\n",
      "Epoch 3, Batch 800, Loss: 1.4350\n",
      "Epoch 3, Batch 900, Loss: 1.4413\n",
      "Epoch 3, Batch 1000, Loss: 1.4628\n",
      "Epoch 3, Batch 1100, Loss: 1.4640\n",
      "Epoch 3, Batch 1200, Loss: 1.4654\n",
      "Epoch 3, Batch 1300, Loss: 1.4506\n",
      "Epoch 3, Batch 1400, Loss: 1.5011\n",
      "Epoch 3, Batch 1500, Loss: 1.4633\n",
      "Epoch 3, Batch 1600, Loss: 1.3837\n",
      "Epoch 3, Batch 1700, Loss: 1.3891\n",
      "Epoch 3, Batch 1800, Loss: 1.4117\n",
      "Epoch 3, Batch 1900, Loss: 1.4093\n",
      "Epoch 3, Batch 2000, Loss: 1.4220\n",
      "Epoch 3, Batch 2100, Loss: 1.3720\n",
      "Epoch 3, Batch 2200, Loss: 1.4670\n",
      "Epoch 3, Batch 2300, Loss: 1.4781\n",
      "Epoch 3, Batch 2400, Loss: 1.4182\n",
      "Epoch 3, Batch 2500, Loss: 1.3968\n",
      "Epoch 3, Batch 2600, Loss: 1.4217\n",
      "Epoch 3, Batch 2700, Loss: 1.3770\n",
      "Epoch 3, Batch 2800, Loss: 1.3847\n",
      "Epoch 3, Batch 2900, Loss: 1.3761\n",
      "Epoch 4, Batch 100, Loss: 1.3530\n",
      "Epoch 4, Batch 200, Loss: 1.3333\n",
      "Epoch 4, Batch 300, Loss: 1.3499\n",
      "Epoch 4, Batch 400, Loss: 1.3817\n",
      "Epoch 4, Batch 500, Loss: 1.3684\n",
      "Epoch 4, Batch 600, Loss: 1.3293\n",
      "Epoch 4, Batch 700, Loss: 1.3638\n",
      "Epoch 4, Batch 800, Loss: 1.3461\n",
      "Epoch 4, Batch 900, Loss: 1.3764\n",
      "Epoch 4, Batch 1000, Loss: 1.2596\n",
      "Epoch 4, Batch 1100, Loss: 1.3040\n",
      "Epoch 4, Batch 1200, Loss: 1.3816\n",
      "Epoch 4, Batch 1300, Loss: 1.3160\n",
      "Epoch 4, Batch 1400, Loss: 1.3090\n",
      "Epoch 4, Batch 1500, Loss: 1.2631\n",
      "Epoch 4, Batch 1600, Loss: 1.3098\n",
      "Epoch 4, Batch 1700, Loss: 1.2986\n",
      "Epoch 4, Batch 1800, Loss: 1.2840\n",
      "Epoch 4, Batch 1900, Loss: 1.2918\n",
      "Epoch 4, Batch 2000, Loss: 1.3047\n",
      "Epoch 4, Batch 2100, Loss: 1.2915\n",
      "Epoch 4, Batch 2200, Loss: 1.3003\n",
      "Epoch 4, Batch 2300, Loss: 1.2662\n",
      "Epoch 4, Batch 2400, Loss: 1.3304\n",
      "Epoch 4, Batch 2500, Loss: 1.2679\n",
      "Epoch 4, Batch 2600, Loss: 1.2723\n",
      "Epoch 4, Batch 2700, Loss: 1.2950\n",
      "Epoch 4, Batch 2800, Loss: 1.3340\n",
      "Epoch 4, Batch 2900, Loss: 1.3086\n",
      "Epoch 5, Batch 100, Loss: 1.2129\n",
      "Epoch 5, Batch 200, Loss: 1.1827\n",
      "Epoch 5, Batch 300, Loss: 1.1976\n",
      "Epoch 5, Batch 400, Loss: 1.2733\n",
      "Epoch 5, Batch 500, Loss: 1.2156\n",
      "Epoch 5, Batch 600, Loss: 1.2038\n",
      "Epoch 5, Batch 700, Loss: 1.2091\n",
      "Epoch 5, Batch 800, Loss: 1.2126\n",
      "Epoch 5, Batch 900, Loss: 1.1873\n",
      "Epoch 5, Batch 1000, Loss: 1.2070\n",
      "Epoch 5, Batch 1100, Loss: 1.2572\n",
      "Epoch 5, Batch 1200, Loss: 1.2515\n",
      "Epoch 5, Batch 1300, Loss: 1.2293\n",
      "Epoch 5, Batch 1400, Loss: 1.1357\n",
      "Epoch 5, Batch 1500, Loss: 1.2214\n",
      "Epoch 5, Batch 1600, Loss: 1.1797\n",
      "Epoch 5, Batch 1700, Loss: 1.2238\n",
      "Epoch 5, Batch 1800, Loss: 1.2115\n",
      "Epoch 5, Batch 1900, Loss: 1.2229\n",
      "Epoch 5, Batch 2000, Loss: 1.1928\n",
      "Epoch 5, Batch 2100, Loss: 1.2596\n",
      "Epoch 5, Batch 2200, Loss: 1.1484\n",
      "Epoch 5, Batch 2300, Loss: 1.2171\n",
      "Epoch 5, Batch 2400, Loss: 1.1887\n",
      "Epoch 5, Batch 2500, Loss: 1.2225\n",
      "Epoch 5, Batch 2600, Loss: 1.2238\n",
      "Epoch 5, Batch 2700, Loss: 1.2137\n",
      "Epoch 5, Batch 2800, Loss: 1.2135\n",
      "Epoch 5, Batch 2900, Loss: 1.2321\n",
      "Epoch 6, Batch 100, Loss: 1.1319\n",
      "Epoch 6, Batch 200, Loss: 1.0755\n",
      "Epoch 6, Batch 300, Loss: 1.1626\n",
      "Epoch 6, Batch 400, Loss: 1.1099\n",
      "Epoch 6, Batch 500, Loss: 1.1524\n",
      "Epoch 6, Batch 600, Loss: 1.0958\n",
      "Epoch 6, Batch 700, Loss: 1.1076\n",
      "Epoch 6, Batch 800, Loss: 1.1331\n",
      "Epoch 6, Batch 900, Loss: 1.0923\n",
      "Epoch 6, Batch 1000, Loss: 1.1230\n",
      "Epoch 6, Batch 1100, Loss: 1.1279\n",
      "Epoch 6, Batch 1200, Loss: 1.1388\n",
      "Epoch 6, Batch 1300, Loss: 1.1260\n",
      "Epoch 6, Batch 1400, Loss: 1.1258\n",
      "Epoch 6, Batch 1500, Loss: 1.0788\n",
      "Epoch 6, Batch 1600, Loss: 1.1243\n",
      "Epoch 6, Batch 1700, Loss: 1.1311\n",
      "Epoch 6, Batch 1800, Loss: 1.1568\n",
      "Epoch 6, Batch 1900, Loss: 1.1089\n",
      "Epoch 6, Batch 2000, Loss: 1.1406\n",
      "Epoch 6, Batch 2100, Loss: 1.0887\n",
      "Epoch 6, Batch 2200, Loss: 1.1760\n",
      "Epoch 6, Batch 2300, Loss: 1.1315\n",
      "Epoch 6, Batch 2400, Loss: 1.1764\n",
      "Epoch 6, Batch 2500, Loss: 1.1138\n",
      "Epoch 6, Batch 2600, Loss: 1.1029\n",
      "Epoch 6, Batch 2700, Loss: 1.1884\n",
      "Epoch 6, Batch 2800, Loss: 1.1197\n",
      "Epoch 6, Batch 2900, Loss: 1.1376\n",
      "Epoch 7, Batch 100, Loss: 1.0671\n",
      "Epoch 7, Batch 200, Loss: 1.0118\n",
      "Epoch 7, Batch 300, Loss: 1.0634\n",
      "Epoch 7, Batch 400, Loss: 1.0384\n",
      "Epoch 7, Batch 500, Loss: 0.9904\n",
      "Epoch 7, Batch 600, Loss: 1.0944\n",
      "Epoch 7, Batch 700, Loss: 1.0469\n",
      "Epoch 7, Batch 800, Loss: 1.0406\n",
      "Epoch 7, Batch 900, Loss: 1.0697\n",
      "Epoch 7, Batch 1000, Loss: 1.0464\n",
      "Epoch 7, Batch 1100, Loss: 1.0533\n",
      "Epoch 7, Batch 1200, Loss: 1.0633\n",
      "Epoch 7, Batch 1300, Loss: 1.0641\n",
      "Epoch 7, Batch 1400, Loss: 1.0510\n",
      "Epoch 7, Batch 1500, Loss: 1.0276\n",
      "Epoch 7, Batch 1600, Loss: 1.0224\n",
      "Epoch 7, Batch 1700, Loss: 1.0873\n",
      "Epoch 7, Batch 1800, Loss: 1.0297\n",
      "Epoch 7, Batch 1900, Loss: 1.0652\n",
      "Epoch 7, Batch 2000, Loss: 1.1104\n",
      "Epoch 7, Batch 2100, Loss: 1.0642\n",
      "Epoch 7, Batch 2200, Loss: 1.1016\n",
      "Epoch 7, Batch 2300, Loss: 1.0577\n",
      "Epoch 7, Batch 2400, Loss: 1.0851\n",
      "Epoch 7, Batch 2500, Loss: 1.0571\n",
      "Epoch 7, Batch 2600, Loss: 1.0839\n",
      "Epoch 7, Batch 2700, Loss: 1.0721\n",
      "Epoch 7, Batch 2800, Loss: 1.0414\n",
      "Epoch 7, Batch 2900, Loss: 1.0483\n",
      "Epoch 8, Batch 100, Loss: 0.9573\n",
      "Epoch 8, Batch 200, Loss: 0.9893\n",
      "Epoch 8, Batch 300, Loss: 1.0313\n",
      "Epoch 8, Batch 400, Loss: 1.0204\n",
      "Epoch 8, Batch 500, Loss: 1.0117\n",
      "Epoch 8, Batch 600, Loss: 0.9668\n",
      "Epoch 8, Batch 700, Loss: 0.9910\n",
      "Epoch 8, Batch 800, Loss: 1.0346\n",
      "Epoch 8, Batch 900, Loss: 1.0097\n",
      "Epoch 8, Batch 1000, Loss: 0.9726\n",
      "Epoch 8, Batch 1100, Loss: 0.9797\n",
      "Epoch 8, Batch 1200, Loss: 0.9831\n",
      "Epoch 8, Batch 1300, Loss: 1.0068\n",
      "Epoch 8, Batch 1400, Loss: 0.9877\n",
      "Epoch 8, Batch 1500, Loss: 0.9624\n",
      "Epoch 8, Batch 1600, Loss: 1.0045\n",
      "Epoch 8, Batch 1700, Loss: 0.9973\n",
      "Epoch 8, Batch 1800, Loss: 1.0120\n",
      "Epoch 8, Batch 1900, Loss: 0.9726\n",
      "Epoch 8, Batch 2000, Loss: 0.9566\n",
      "Epoch 8, Batch 2100, Loss: 1.0079\n",
      "Epoch 8, Batch 2200, Loss: 0.9737\n",
      "Epoch 8, Batch 2300, Loss: 1.0100\n",
      "Epoch 8, Batch 2400, Loss: 0.9712\n",
      "Epoch 8, Batch 2500, Loss: 1.0318\n",
      "Epoch 8, Batch 2600, Loss: 1.0748\n",
      "Epoch 8, Batch 2700, Loss: 1.0151\n",
      "Epoch 8, Batch 2800, Loss: 1.0130\n",
      "Epoch 8, Batch 2900, Loss: 0.9856\n",
      "Epoch 9, Batch 100, Loss: 0.9203\n",
      "Epoch 9, Batch 200, Loss: 0.9352\n",
      "Epoch 9, Batch 300, Loss: 0.8901\n",
      "Epoch 9, Batch 400, Loss: 0.9469\n",
      "Epoch 9, Batch 500, Loss: 0.9445\n",
      "Epoch 9, Batch 600, Loss: 0.9436\n",
      "Epoch 9, Batch 700, Loss: 0.9142\n",
      "Epoch 9, Batch 800, Loss: 0.9527\n",
      "Epoch 9, Batch 900, Loss: 0.9225\n",
      "Epoch 9, Batch 1000, Loss: 0.9244\n",
      "Epoch 9, Batch 1100, Loss: 0.9353\n",
      "Epoch 9, Batch 1200, Loss: 0.9065\n",
      "Epoch 9, Batch 1300, Loss: 0.9473\n",
      "Epoch 9, Batch 1400, Loss: 0.9492\n",
      "Epoch 9, Batch 1500, Loss: 0.9508\n",
      "Epoch 9, Batch 1600, Loss: 0.9259\n",
      "Epoch 9, Batch 1700, Loss: 0.9840\n",
      "Epoch 9, Batch 1800, Loss: 0.9484\n",
      "Epoch 9, Batch 1900, Loss: 0.9521\n",
      "Epoch 9, Batch 2000, Loss: 0.9192\n",
      "Epoch 9, Batch 2100, Loss: 0.9675\n",
      "Epoch 9, Batch 2200, Loss: 0.9227\n",
      "Epoch 9, Batch 2300, Loss: 0.9795\n",
      "Epoch 9, Batch 2400, Loss: 0.9752\n",
      "Epoch 9, Batch 2500, Loss: 0.9598\n",
      "Epoch 9, Batch 2600, Loss: 0.9349\n",
      "Epoch 9, Batch 2700, Loss: 0.9701\n",
      "Epoch 9, Batch 2800, Loss: 0.9252\n",
      "Epoch 9, Batch 2900, Loss: 0.9647\n",
      "Epoch 10, Batch 100, Loss: 0.9033\n",
      "Epoch 10, Batch 200, Loss: 0.8655\n",
      "Epoch 10, Batch 300, Loss: 0.8683\n",
      "Epoch 10, Batch 400, Loss: 0.8572\n",
      "Epoch 10, Batch 500, Loss: 0.8862\n",
      "Epoch 10, Batch 600, Loss: 0.8896\n",
      "Epoch 10, Batch 700, Loss: 0.8688\n",
      "Epoch 10, Batch 800, Loss: 0.8342\n",
      "Epoch 10, Batch 900, Loss: 0.9180\n",
      "Epoch 10, Batch 1000, Loss: 0.8776\n",
      "Epoch 10, Batch 1100, Loss: 0.8496\n",
      "Epoch 10, Batch 1200, Loss: 0.8756\n",
      "Epoch 10, Batch 1300, Loss: 0.8770\n",
      "Epoch 10, Batch 1400, Loss: 0.8833\n",
      "Epoch 10, Batch 1500, Loss: 0.8893\n",
      "Epoch 10, Batch 1600, Loss: 0.8673\n",
      "Epoch 10, Batch 1700, Loss: 0.8592\n",
      "Epoch 10, Batch 1800, Loss: 0.8985\n",
      "Epoch 10, Batch 1900, Loss: 0.9374\n",
      "Epoch 10, Batch 2000, Loss: 0.8719\n",
      "Epoch 10, Batch 2100, Loss: 0.8859\n",
      "Epoch 10, Batch 2200, Loss: 0.9141\n",
      "Epoch 10, Batch 2300, Loss: 0.9142\n",
      "Epoch 10, Batch 2400, Loss: 0.9392\n",
      "Epoch 10, Batch 2500, Loss: 0.9066\n",
      "Epoch 10, Batch 2600, Loss: 0.9238\n",
      "Epoch 10, Batch 2700, Loss: 0.9257\n",
      "Epoch 10, Batch 2800, Loss: 0.8582\n",
      "Epoch 10, Batch 2900, Loss: 0.9359\n",
      "Epoch 11, Batch 100, Loss: 0.8098\n",
      "Epoch 11, Batch 200, Loss: 0.8345\n",
      "Epoch 11, Batch 300, Loss: 0.8223\n",
      "Epoch 11, Batch 400, Loss: 0.8234\n",
      "Epoch 11, Batch 500, Loss: 0.8282\n",
      "Epoch 11, Batch 600, Loss: 0.8140\n",
      "Epoch 11, Batch 700, Loss: 0.8497\n",
      "Epoch 11, Batch 800, Loss: 0.8261\n",
      "Epoch 11, Batch 900, Loss: 0.8190\n",
      "Epoch 11, Batch 1000, Loss: 0.8642\n",
      "Epoch 11, Batch 1100, Loss: 0.8261\n",
      "Epoch 11, Batch 1200, Loss: 0.8578\n",
      "Epoch 11, Batch 1300, Loss: 0.8811\n",
      "Epoch 11, Batch 1400, Loss: 0.8538\n",
      "Epoch 11, Batch 1500, Loss: 0.8705\n",
      "Epoch 11, Batch 1600, Loss: 0.8255\n",
      "Epoch 11, Batch 1700, Loss: 0.8182\n",
      "Epoch 11, Batch 1800, Loss: 0.8392\n",
      "Epoch 11, Batch 1900, Loss: 0.8856\n",
      "Epoch 11, Batch 2000, Loss: 0.8627\n",
      "Epoch 11, Batch 2100, Loss: 0.8424\n",
      "Epoch 11, Batch 2200, Loss: 0.8652\n",
      "Epoch 11, Batch 2300, Loss: 0.8209\n",
      "Epoch 11, Batch 2400, Loss: 0.8593\n",
      "Epoch 11, Batch 2500, Loss: 0.8507\n",
      "Epoch 11, Batch 2600, Loss: 0.8547\n",
      "Epoch 11, Batch 2700, Loss: 0.8596\n",
      "Epoch 11, Batch 2800, Loss: 0.8991\n",
      "Epoch 11, Batch 2900, Loss: 0.8033\n",
      "Epoch 12, Batch 100, Loss: 0.7455\n",
      "Epoch 12, Batch 200, Loss: 0.7497\n",
      "Epoch 12, Batch 300, Loss: 0.7698\n",
      "Epoch 12, Batch 400, Loss: 0.7860\n",
      "Epoch 12, Batch 500, Loss: 0.8075\n",
      "Epoch 12, Batch 600, Loss: 0.7635\n",
      "Epoch 12, Batch 700, Loss: 0.7728\n",
      "Epoch 12, Batch 800, Loss: 0.8151\n",
      "Epoch 12, Batch 900, Loss: 0.7541\n",
      "Epoch 12, Batch 1000, Loss: 0.7895\n",
      "Epoch 12, Batch 1100, Loss: 0.8070\n",
      "Epoch 12, Batch 1200, Loss: 0.7899\n",
      "Epoch 12, Batch 1300, Loss: 0.7736\n",
      "Epoch 12, Batch 1400, Loss: 0.7907\n",
      "Epoch 12, Batch 1500, Loss: 0.8150\n",
      "Epoch 12, Batch 1600, Loss: 0.7945\n",
      "Epoch 12, Batch 1700, Loss: 0.8136\n",
      "Epoch 12, Batch 1800, Loss: 0.8128\n",
      "Epoch 12, Batch 1900, Loss: 0.8025\n",
      "Epoch 12, Batch 2000, Loss: 0.7887\n",
      "Epoch 12, Batch 2100, Loss: 0.8071\n",
      "Epoch 12, Batch 2200, Loss: 0.8289\n",
      "Epoch 12, Batch 2300, Loss: 0.8266\n",
      "Epoch 12, Batch 2400, Loss: 0.8367\n",
      "Epoch 12, Batch 2500, Loss: 0.8176\n",
      "Epoch 12, Batch 2600, Loss: 0.7853\n",
      "Epoch 12, Batch 2700, Loss: 0.8202\n",
      "Epoch 12, Batch 2800, Loss: 0.8099\n",
      "Epoch 12, Batch 2900, Loss: 0.7941\n",
      "Epoch 13, Batch 100, Loss: 0.7491\n",
      "Epoch 13, Batch 200, Loss: 0.7533\n",
      "Epoch 13, Batch 300, Loss: 0.7598\n",
      "Epoch 13, Batch 400, Loss: 0.7483\n",
      "Epoch 13, Batch 500, Loss: 0.7575\n",
      "Epoch 13, Batch 600, Loss: 0.7280\n",
      "Epoch 13, Batch 700, Loss: 0.7359\n",
      "Epoch 13, Batch 800, Loss: 0.7649\n",
      "Epoch 13, Batch 900, Loss: 0.7400\n",
      "Epoch 13, Batch 1000, Loss: 0.7434\n",
      "Epoch 13, Batch 1100, Loss: 0.7880\n",
      "Epoch 13, Batch 1200, Loss: 0.7589\n",
      "Epoch 13, Batch 1300, Loss: 0.7343\n",
      "Epoch 13, Batch 1400, Loss: 0.7710\n",
      "Epoch 13, Batch 1500, Loss: 0.7652\n",
      "Epoch 13, Batch 1600, Loss: 0.7350\n",
      "Epoch 13, Batch 1700, Loss: 0.7649\n",
      "Epoch 13, Batch 1800, Loss: 0.7610\n",
      "Epoch 13, Batch 1900, Loss: 0.7504\n",
      "Epoch 13, Batch 2000, Loss: 0.7505\n",
      "Epoch 13, Batch 2100, Loss: 0.7412\n",
      "Epoch 13, Batch 2200, Loss: 0.7664\n",
      "Epoch 13, Batch 2300, Loss: 0.7461\n",
      "Epoch 13, Batch 2400, Loss: 0.7314\n",
      "Epoch 13, Batch 2500, Loss: 0.7435\n",
      "Epoch 13, Batch 2600, Loss: 0.7263\n",
      "Epoch 13, Batch 2700, Loss: 0.7393\n",
      "Epoch 13, Batch 2800, Loss: 0.7589\n",
      "Epoch 13, Batch 2900, Loss: 0.7722\n",
      "Epoch 14, Batch 100, Loss: 0.6812\n",
      "Epoch 14, Batch 200, Loss: 0.6846\n",
      "Epoch 14, Batch 300, Loss: 0.6864\n",
      "Epoch 14, Batch 400, Loss: 0.7177\n",
      "Epoch 14, Batch 500, Loss: 0.7099\n",
      "Epoch 14, Batch 600, Loss: 0.6697\n",
      "Epoch 14, Batch 700, Loss: 0.7011\n",
      "Epoch 14, Batch 800, Loss: 0.6916\n",
      "Epoch 14, Batch 900, Loss: 0.6757\n",
      "Epoch 14, Batch 1000, Loss: 0.7259\n",
      "Epoch 14, Batch 1100, Loss: 0.7474\n",
      "Epoch 14, Batch 1200, Loss: 0.7017\n",
      "Epoch 14, Batch 1300, Loss: 0.7316\n",
      "Epoch 14, Batch 1400, Loss: 0.7004\n",
      "Epoch 14, Batch 1500, Loss: 0.7369\n",
      "Epoch 14, Batch 1600, Loss: 0.6613\n",
      "Epoch 14, Batch 1700, Loss: 0.6972\n",
      "Epoch 14, Batch 1800, Loss: 0.6802\n",
      "Epoch 14, Batch 1900, Loss: 0.7121\n",
      "Epoch 14, Batch 2000, Loss: 0.7622\n",
      "Epoch 14, Batch 2100, Loss: 0.7159\n",
      "Epoch 14, Batch 2200, Loss: 0.6918\n",
      "Epoch 14, Batch 2300, Loss: 0.7211\n",
      "Epoch 14, Batch 2400, Loss: 0.7152\n",
      "Epoch 14, Batch 2500, Loss: 0.7248\n",
      "Epoch 14, Batch 2600, Loss: 0.7021\n",
      "Epoch 14, Batch 2700, Loss: 0.7609\n",
      "Epoch 14, Batch 2800, Loss: 0.7155\n",
      "Epoch 14, Batch 2900, Loss: 0.7430\n",
      "Epoch 15, Batch 100, Loss: 0.6063\n",
      "Epoch 15, Batch 200, Loss: 0.6745\n",
      "Epoch 15, Batch 300, Loss: 0.6590\n",
      "Epoch 15, Batch 400, Loss: 0.6443\n",
      "Epoch 15, Batch 500, Loss: 0.6656\n",
      "Epoch 15, Batch 600, Loss: 0.6541\n",
      "Epoch 15, Batch 700, Loss: 0.6754\n",
      "Epoch 15, Batch 800, Loss: 0.6608\n",
      "Epoch 15, Batch 900, Loss: 0.6691\n",
      "Epoch 15, Batch 1000, Loss: 0.6378\n",
      "Epoch 15, Batch 1100, Loss: 0.6371\n",
      "Epoch 15, Batch 1200, Loss: 0.6587\n",
      "Epoch 15, Batch 1300, Loss: 0.6571\n",
      "Epoch 15, Batch 1400, Loss: 0.6575\n",
      "Epoch 15, Batch 1500, Loss: 0.6665\n",
      "Epoch 15, Batch 1600, Loss: 0.6681\n",
      "Epoch 15, Batch 1700, Loss: 0.6641\n",
      "Epoch 15, Batch 1800, Loss: 0.6759\n",
      "Epoch 15, Batch 1900, Loss: 0.6693\n",
      "Epoch 15, Batch 2000, Loss: 0.7054\n",
      "Epoch 15, Batch 2100, Loss: 0.6971\n",
      "Epoch 15, Batch 2200, Loss: 0.6756\n",
      "Epoch 15, Batch 2300, Loss: 0.6673\n",
      "Epoch 15, Batch 2400, Loss: 0.6603\n",
      "Epoch 15, Batch 2500, Loss: 0.6942\n",
      "Epoch 15, Batch 2600, Loss: 0.6766\n",
      "Epoch 15, Batch 2700, Loss: 0.7176\n",
      "Epoch 15, Batch 2800, Loss: 0.7105\n",
      "Epoch 15, Batch 2900, Loss: 0.6732\n",
      "Epoch 16, Batch 100, Loss: 0.6030\n",
      "Epoch 16, Batch 200, Loss: 0.5995\n",
      "Epoch 16, Batch 300, Loss: 0.6162\n",
      "Epoch 16, Batch 400, Loss: 0.5986\n",
      "Epoch 16, Batch 500, Loss: 0.6268\n",
      "Epoch 16, Batch 600, Loss: 0.6065\n",
      "Epoch 16, Batch 700, Loss: 0.6225\n",
      "Epoch 16, Batch 800, Loss: 0.6270\n",
      "Epoch 16, Batch 900, Loss: 0.6537\n",
      "Epoch 16, Batch 1000, Loss: 0.6372\n",
      "Epoch 16, Batch 1100, Loss: 0.6513\n",
      "Epoch 16, Batch 1200, Loss: 0.6061\n",
      "Epoch 16, Batch 1300, Loss: 0.6125\n",
      "Epoch 16, Batch 1400, Loss: 0.6236\n",
      "Epoch 16, Batch 1500, Loss: 0.6070\n",
      "Epoch 16, Batch 1600, Loss: 0.6081\n",
      "Epoch 16, Batch 1700, Loss: 0.6729\n",
      "Epoch 16, Batch 1800, Loss: 0.6753\n",
      "Epoch 16, Batch 1900, Loss: 0.6295\n",
      "Epoch 16, Batch 2000, Loss: 0.6677\n",
      "Epoch 16, Batch 2100, Loss: 0.6707\n",
      "Epoch 16, Batch 2200, Loss: 0.6499\n",
      "Epoch 16, Batch 2300, Loss: 0.6327\n",
      "Epoch 16, Batch 2400, Loss: 0.6609\n",
      "Epoch 16, Batch 2500, Loss: 0.6203\n",
      "Epoch 16, Batch 2600, Loss: 0.6456\n",
      "Epoch 16, Batch 2700, Loss: 0.6450\n",
      "Epoch 16, Batch 2800, Loss: 0.6836\n",
      "Epoch 16, Batch 2900, Loss: 0.6888\n",
      "Epoch 17, Batch 100, Loss: 0.5912\n",
      "Epoch 17, Batch 200, Loss: 0.5929\n",
      "Epoch 17, Batch 300, Loss: 0.5682\n",
      "Epoch 17, Batch 400, Loss: 0.5661\n",
      "Epoch 17, Batch 500, Loss: 0.6192\n",
      "Epoch 17, Batch 600, Loss: 0.6081\n",
      "Epoch 17, Batch 700, Loss: 0.5668\n",
      "Epoch 17, Batch 800, Loss: 0.6057\n",
      "Epoch 17, Batch 900, Loss: 0.5882\n",
      "Epoch 17, Batch 1000, Loss: 0.5706\n",
      "Epoch 17, Batch 1100, Loss: 0.6110\n",
      "Epoch 17, Batch 1200, Loss: 0.5934\n",
      "Epoch 17, Batch 1300, Loss: 0.5841\n",
      "Epoch 17, Batch 1400, Loss: 0.6055\n",
      "Epoch 17, Batch 1500, Loss: 0.5721\n",
      "Epoch 17, Batch 1600, Loss: 0.6059\n",
      "Epoch 17, Batch 1700, Loss: 0.6044\n",
      "Epoch 17, Batch 1800, Loss: 0.6117\n",
      "Epoch 17, Batch 1900, Loss: 0.6109\n",
      "Epoch 17, Batch 2000, Loss: 0.6143\n",
      "Epoch 17, Batch 2100, Loss: 0.6286\n",
      "Epoch 17, Batch 2200, Loss: 0.5969\n",
      "Epoch 17, Batch 2300, Loss: 0.6186\n",
      "Epoch 17, Batch 2400, Loss: 0.6095\n",
      "Epoch 17, Batch 2500, Loss: 0.5952\n",
      "Epoch 17, Batch 2600, Loss: 0.6206\n",
      "Epoch 17, Batch 2700, Loss: 0.6205\n",
      "Epoch 17, Batch 2800, Loss: 0.6369\n",
      "Epoch 17, Batch 2900, Loss: 0.6482\n",
      "Epoch 18, Batch 100, Loss: 0.5681\n",
      "Epoch 18, Batch 200, Loss: 0.5174\n",
      "Epoch 18, Batch 300, Loss: 0.5786\n",
      "Epoch 18, Batch 400, Loss: 0.5525\n",
      "Epoch 18, Batch 500, Loss: 0.5462\n",
      "Epoch 18, Batch 600, Loss: 0.5463\n",
      "Epoch 18, Batch 700, Loss: 0.5560\n",
      "Epoch 18, Batch 800, Loss: 0.5678\n",
      "Epoch 18, Batch 900, Loss: 0.5886\n",
      "Epoch 18, Batch 1000, Loss: 0.5711\n",
      "Epoch 18, Batch 1100, Loss: 0.5442\n",
      "Epoch 18, Batch 1200, Loss: 0.5972\n",
      "Epoch 18, Batch 1300, Loss: 0.5771\n",
      "Epoch 18, Batch 1400, Loss: 0.5792\n",
      "Epoch 18, Batch 1500, Loss: 0.5809\n",
      "Epoch 18, Batch 1600, Loss: 0.5849\n",
      "Epoch 18, Batch 1700, Loss: 0.5795\n",
      "Epoch 18, Batch 1800, Loss: 0.5693\n",
      "Epoch 18, Batch 1900, Loss: 0.5470\n",
      "Epoch 18, Batch 2000, Loss: 0.5536\n",
      "Epoch 18, Batch 2100, Loss: 0.6143\n",
      "Epoch 18, Batch 2200, Loss: 0.5870\n",
      "Epoch 18, Batch 2300, Loss: 0.5750\n",
      "Epoch 18, Batch 2400, Loss: 0.6011\n",
      "Epoch 18, Batch 2500, Loss: 0.5771\n",
      "Epoch 18, Batch 2600, Loss: 0.5890\n",
      "Epoch 18, Batch 2700, Loss: 0.5967\n",
      "Epoch 18, Batch 2800, Loss: 0.5712\n",
      "Epoch 18, Batch 2900, Loss: 0.5837\n",
      "Epoch 19, Batch 100, Loss: 0.5391\n",
      "Epoch 19, Batch 200, Loss: 0.5487\n",
      "Epoch 19, Batch 300, Loss: 0.5579\n",
      "Epoch 19, Batch 400, Loss: 0.5299\n",
      "Epoch 19, Batch 500, Loss: 0.5668\n",
      "Epoch 19, Batch 600, Loss: 0.5431\n",
      "Epoch 19, Batch 700, Loss: 0.5151\n",
      "Epoch 19, Batch 800, Loss: 0.5544\n",
      "Epoch 19, Batch 900, Loss: 0.5293\n",
      "Epoch 19, Batch 1000, Loss: 0.5256\n",
      "Epoch 19, Batch 1100, Loss: 0.5254\n",
      "Epoch 19, Batch 1200, Loss: 0.5309\n",
      "Epoch 19, Batch 1300, Loss: 0.5146\n",
      "Epoch 19, Batch 1400, Loss: 0.5536\n",
      "Epoch 19, Batch 1500, Loss: 0.5463\n",
      "Epoch 19, Batch 1600, Loss: 0.5340\n",
      "Epoch 19, Batch 1700, Loss: 0.5313\n",
      "Epoch 19, Batch 1800, Loss: 0.5496\n",
      "Epoch 19, Batch 1900, Loss: 0.5443\n",
      "Epoch 19, Batch 2000, Loss: 0.5379\n",
      "Epoch 19, Batch 2100, Loss: 0.5230\n",
      "Epoch 19, Batch 2200, Loss: 0.5077\n",
      "Epoch 19, Batch 2300, Loss: 0.5567\n",
      "Epoch 19, Batch 2400, Loss: 0.5721\n",
      "Epoch 19, Batch 2500, Loss: 0.5915\n",
      "Epoch 19, Batch 2600, Loss: 0.5518\n",
      "Epoch 19, Batch 2700, Loss: 0.5603\n",
      "Epoch 19, Batch 2800, Loss: 0.5416\n",
      "Epoch 19, Batch 2900, Loss: 0.5570\n",
      "Epoch 20, Batch 100, Loss: 0.4917\n",
      "Epoch 20, Batch 200, Loss: 0.5000\n",
      "Epoch 20, Batch 300, Loss: 0.5117\n",
      "Epoch 20, Batch 400, Loss: 0.5050\n",
      "Epoch 20, Batch 500, Loss: 0.5038\n",
      "Epoch 20, Batch 600, Loss: 0.5136\n",
      "Epoch 20, Batch 700, Loss: 0.4928\n",
      "Epoch 20, Batch 800, Loss: 0.5017\n",
      "Epoch 20, Batch 900, Loss: 0.5089\n",
      "Epoch 20, Batch 1000, Loss: 0.5237\n",
      "Epoch 20, Batch 1100, Loss: 0.5151\n",
      "Epoch 20, Batch 1200, Loss: 0.5146\n",
      "Epoch 20, Batch 1300, Loss: 0.5219\n",
      "Epoch 20, Batch 1400, Loss: 0.5185\n",
      "Epoch 20, Batch 1500, Loss: 0.4943\n",
      "Epoch 20, Batch 1600, Loss: 0.5409\n",
      "Epoch 20, Batch 1700, Loss: 0.5044\n",
      "Epoch 20, Batch 1800, Loss: 0.5124\n",
      "Epoch 20, Batch 1900, Loss: 0.5294\n",
      "Epoch 20, Batch 2000, Loss: 0.5352\n",
      "Epoch 20, Batch 2100, Loss: 0.5240\n",
      "Epoch 20, Batch 2200, Loss: 0.5121\n",
      "Epoch 20, Batch 2300, Loss: 0.5483\n",
      "Epoch 20, Batch 2400, Loss: 0.5557\n",
      "Epoch 20, Batch 2500, Loss: 0.5450\n",
      "Epoch 20, Batch 2600, Loss: 0.5082\n",
      "Epoch 20, Batch 2700, Loss: 0.5073\n",
      "Epoch 20, Batch 2800, Loss: 0.5459\n",
      "Epoch 20, Batch 2900, Loss: 0.5141\n",
      "Epoch 21, Batch 100, Loss: 0.4814\n",
      "Epoch 21, Batch 200, Loss: 0.4727\n",
      "Epoch 21, Batch 300, Loss: 0.4653\n",
      "Epoch 21, Batch 400, Loss: 0.4855\n",
      "Epoch 21, Batch 500, Loss: 0.4775\n",
      "Epoch 21, Batch 600, Loss: 0.5177\n",
      "Epoch 21, Batch 700, Loss: 0.4736\n",
      "Epoch 21, Batch 800, Loss: 0.4787\n",
      "Epoch 21, Batch 900, Loss: 0.4616\n",
      "Epoch 21, Batch 1000, Loss: 0.4845\n",
      "Epoch 21, Batch 1100, Loss: 0.4573\n",
      "Epoch 21, Batch 1200, Loss: 0.4991\n",
      "Epoch 21, Batch 1300, Loss: 0.4905\n",
      "Epoch 21, Batch 1400, Loss: 0.5101\n",
      "Epoch 21, Batch 1500, Loss: 0.5140\n",
      "Epoch 21, Batch 1600, Loss: 0.4820\n",
      "Epoch 21, Batch 1700, Loss: 0.4912\n",
      "Epoch 21, Batch 1800, Loss: 0.5028\n",
      "Epoch 21, Batch 1900, Loss: 0.5049\n",
      "Epoch 21, Batch 2000, Loss: 0.4987\n",
      "Epoch 21, Batch 2100, Loss: 0.4730\n",
      "Epoch 21, Batch 2200, Loss: 0.5210\n",
      "Epoch 21, Batch 2300, Loss: 0.5249\n",
      "Epoch 21, Batch 2400, Loss: 0.5329\n",
      "Epoch 21, Batch 2500, Loss: 0.5201\n",
      "Epoch 21, Batch 2600, Loss: 0.5181\n",
      "Epoch 21, Batch 2700, Loss: 0.5021\n",
      "Epoch 21, Batch 2800, Loss: 0.5142\n",
      "Epoch 21, Batch 2900, Loss: 0.5174\n",
      "Epoch 22, Batch 100, Loss: 0.5062\n",
      "Epoch 22, Batch 200, Loss: 0.4490\n",
      "Epoch 22, Batch 300, Loss: 0.4336\n",
      "Epoch 22, Batch 400, Loss: 0.4988\n",
      "Epoch 22, Batch 500, Loss: 0.4550\n",
      "Epoch 22, Batch 600, Loss: 0.4544\n",
      "Epoch 22, Batch 700, Loss: 0.4723\n",
      "Epoch 22, Batch 800, Loss: 0.4985\n",
      "Epoch 22, Batch 900, Loss: 0.4574\n",
      "Epoch 22, Batch 1000, Loss: 0.4688\n",
      "Epoch 22, Batch 1100, Loss: 0.4748\n",
      "Epoch 22, Batch 1200, Loss: 0.4622\n",
      "Epoch 22, Batch 1300, Loss: 0.4853\n",
      "Epoch 22, Batch 1400, Loss: 0.4575\n",
      "Epoch 22, Batch 1500, Loss: 0.4643\n",
      "Epoch 22, Batch 1600, Loss: 0.4595\n",
      "Epoch 22, Batch 1700, Loss: 0.4803\n",
      "Epoch 22, Batch 1800, Loss: 0.5069\n",
      "Epoch 22, Batch 1900, Loss: 0.4878\n",
      "Epoch 22, Batch 2000, Loss: 0.5084\n",
      "Epoch 22, Batch 2100, Loss: 0.4776\n",
      "Epoch 22, Batch 2200, Loss: 0.4671\n",
      "Epoch 22, Batch 2300, Loss: 0.4844\n",
      "Epoch 22, Batch 2400, Loss: 0.4617\n",
      "Epoch 22, Batch 2500, Loss: 0.5174\n",
      "Epoch 22, Batch 2600, Loss: 0.4638\n",
      "Epoch 22, Batch 2700, Loss: 0.4576\n",
      "Epoch 22, Batch 2800, Loss: 0.4750\n",
      "Epoch 22, Batch 2900, Loss: 0.4985\n",
      "Epoch 23, Batch 100, Loss: 0.4598\n",
      "Epoch 23, Batch 200, Loss: 0.4378\n",
      "Epoch 23, Batch 300, Loss: 0.4368\n",
      "Epoch 23, Batch 400, Loss: 0.4659\n",
      "Epoch 23, Batch 500, Loss: 0.4370\n",
      "Epoch 23, Batch 600, Loss: 0.4518\n",
      "Epoch 23, Batch 700, Loss: 0.4343\n",
      "Epoch 23, Batch 800, Loss: 0.4262\n",
      "Epoch 23, Batch 900, Loss: 0.4590\n",
      "Epoch 23, Batch 1000, Loss: 0.4408\n",
      "Epoch 23, Batch 1100, Loss: 0.4642\n",
      "Epoch 23, Batch 1200, Loss: 0.4601\n",
      "Epoch 23, Batch 1300, Loss: 0.4622\n",
      "Epoch 23, Batch 1400, Loss: 0.4575\n",
      "Epoch 23, Batch 1500, Loss: 0.4607\n",
      "Epoch 23, Batch 1600, Loss: 0.4553\n",
      "Epoch 23, Batch 1700, Loss: 0.4385\n",
      "Epoch 23, Batch 1800, Loss: 0.4459\n",
      "Epoch 23, Batch 1900, Loss: 0.4459\n",
      "Epoch 23, Batch 2000, Loss: 0.4488\n",
      "Epoch 23, Batch 2100, Loss: 0.4569\n",
      "Epoch 23, Batch 2200, Loss: 0.4641\n",
      "Epoch 23, Batch 2300, Loss: 0.4738\n",
      "Epoch 23, Batch 2400, Loss: 0.4827\n",
      "Epoch 23, Batch 2500, Loss: 0.4523\n",
      "Epoch 23, Batch 2600, Loss: 0.4563\n",
      "Epoch 23, Batch 2700, Loss: 0.4682\n",
      "Epoch 23, Batch 2800, Loss: 0.4658\n",
      "Epoch 23, Batch 2900, Loss: 0.4791\n",
      "Epoch 24, Batch 100, Loss: 0.4038\n",
      "Epoch 24, Batch 200, Loss: 0.4194\n",
      "Epoch 24, Batch 300, Loss: 0.4288\n",
      "Epoch 24, Batch 400, Loss: 0.4326\n",
      "Epoch 24, Batch 500, Loss: 0.4259\n",
      "Epoch 24, Batch 600, Loss: 0.4132\n",
      "Epoch 24, Batch 700, Loss: 0.4120\n",
      "Epoch 24, Batch 800, Loss: 0.4491\n",
      "Epoch 24, Batch 900, Loss: 0.4329\n",
      "Epoch 24, Batch 1000, Loss: 0.4187\n",
      "Epoch 24, Batch 1100, Loss: 0.4264\n",
      "Epoch 24, Batch 1200, Loss: 0.4352\n",
      "Epoch 24, Batch 1300, Loss: 0.4405\n",
      "Epoch 24, Batch 1400, Loss: 0.4205\n",
      "Epoch 24, Batch 1500, Loss: 0.4267\n",
      "Epoch 24, Batch 1600, Loss: 0.4384\n",
      "Epoch 24, Batch 1700, Loss: 0.4238\n",
      "Epoch 24, Batch 1800, Loss: 0.4173\n",
      "Epoch 24, Batch 1900, Loss: 0.4320\n",
      "Epoch 24, Batch 2000, Loss: 0.4527\n",
      "Epoch 24, Batch 2100, Loss: 0.4267\n",
      "Epoch 24, Batch 2200, Loss: 0.4442\n",
      "Epoch 24, Batch 2300, Loss: 0.4591\n",
      "Epoch 24, Batch 2400, Loss: 0.4431\n",
      "Epoch 24, Batch 2500, Loss: 0.4527\n",
      "Epoch 24, Batch 2600, Loss: 0.4391\n",
      "Epoch 24, Batch 2700, Loss: 0.4546\n",
      "Epoch 24, Batch 2800, Loss: 0.4389\n",
      "Epoch 24, Batch 2900, Loss: 0.4555\n",
      "Epoch 25, Batch 100, Loss: 0.4044\n",
      "Epoch 25, Batch 200, Loss: 0.3984\n",
      "Epoch 25, Batch 300, Loss: 0.4123\n",
      "Epoch 25, Batch 400, Loss: 0.4098\n",
      "Epoch 25, Batch 500, Loss: 0.3836\n",
      "Epoch 25, Batch 600, Loss: 0.4136\n",
      "Epoch 25, Batch 700, Loss: 0.4082\n",
      "Epoch 25, Batch 800, Loss: 0.4228\n",
      "Epoch 25, Batch 900, Loss: 0.4113\n",
      "Epoch 25, Batch 1000, Loss: 0.4025\n",
      "Epoch 25, Batch 1100, Loss: 0.4142\n",
      "Epoch 25, Batch 1200, Loss: 0.4079\n",
      "Epoch 25, Batch 1300, Loss: 0.4239\n",
      "Epoch 25, Batch 1400, Loss: 0.4118\n",
      "Epoch 25, Batch 1500, Loss: 0.4230\n",
      "Epoch 25, Batch 1600, Loss: 0.4321\n",
      "Epoch 25, Batch 1700, Loss: 0.4653\n",
      "Epoch 25, Batch 1800, Loss: 0.4080\n",
      "Epoch 25, Batch 1900, Loss: 0.4236\n",
      "Epoch 25, Batch 2000, Loss: 0.4322\n",
      "Epoch 25, Batch 2100, Loss: 0.4352\n",
      "Epoch 25, Batch 2200, Loss: 0.4292\n",
      "Epoch 25, Batch 2300, Loss: 0.4061\n",
      "Epoch 25, Batch 2400, Loss: 0.4472\n",
      "Epoch 25, Batch 2500, Loss: 0.4314\n",
      "Epoch 25, Batch 2600, Loss: 0.4543\n",
      "Epoch 25, Batch 2700, Loss: 0.4337\n",
      "Epoch 25, Batch 2800, Loss: 0.4510\n",
      "Epoch 25, Batch 2900, Loss: 0.4360\n",
      "Epoch 26, Batch 100, Loss: 0.4121\n",
      "Epoch 26, Batch 200, Loss: 0.3905\n",
      "Epoch 26, Batch 300, Loss: 0.3925\n",
      "Epoch 26, Batch 400, Loss: 0.4003\n",
      "Epoch 26, Batch 500, Loss: 0.3735\n",
      "Epoch 26, Batch 600, Loss: 0.3850\n",
      "Epoch 26, Batch 700, Loss: 0.4068\n",
      "Epoch 26, Batch 800, Loss: 0.3844\n",
      "Epoch 26, Batch 900, Loss: 0.4043\n",
      "Epoch 26, Batch 1000, Loss: 0.4107\n",
      "Epoch 26, Batch 1100, Loss: 0.3968\n",
      "Epoch 26, Batch 1200, Loss: 0.3941\n",
      "Epoch 26, Batch 1300, Loss: 0.4117\n",
      "Epoch 26, Batch 1400, Loss: 0.4058\n",
      "Epoch 26, Batch 1500, Loss: 0.4158\n",
      "Epoch 26, Batch 1600, Loss: 0.3941\n",
      "Epoch 26, Batch 1700, Loss: 0.3976\n",
      "Epoch 26, Batch 1800, Loss: 0.3980\n",
      "Epoch 26, Batch 1900, Loss: 0.4281\n",
      "Epoch 26, Batch 2000, Loss: 0.4198\n",
      "Epoch 26, Batch 2100, Loss: 0.4092\n",
      "Epoch 26, Batch 2200, Loss: 0.4083\n",
      "Epoch 26, Batch 2300, Loss: 0.3963\n",
      "Epoch 26, Batch 2400, Loss: 0.4359\n",
      "Epoch 26, Batch 2500, Loss: 0.4179\n",
      "Epoch 26, Batch 2600, Loss: 0.4306\n",
      "Epoch 26, Batch 2700, Loss: 0.4157\n",
      "Epoch 26, Batch 2800, Loss: 0.4091\n",
      "Epoch 26, Batch 2900, Loss: 0.4096\n",
      "Epoch 27, Batch 100, Loss: 0.3883\n",
      "Epoch 27, Batch 200, Loss: 0.3961\n",
      "Epoch 27, Batch 300, Loss: 0.3715\n",
      "Epoch 27, Batch 400, Loss: 0.3810\n",
      "Epoch 27, Batch 500, Loss: 0.3838\n",
      "Epoch 27, Batch 600, Loss: 0.3979\n",
      "Epoch 27, Batch 700, Loss: 0.3652\n",
      "Epoch 27, Batch 800, Loss: 0.3790\n",
      "Epoch 27, Batch 900, Loss: 0.3794\n",
      "Epoch 27, Batch 1000, Loss: 0.3619\n",
      "Epoch 27, Batch 1100, Loss: 0.3918\n",
      "Epoch 27, Batch 1200, Loss: 0.3969\n",
      "Epoch 27, Batch 1300, Loss: 0.3858\n",
      "Epoch 27, Batch 1400, Loss: 0.3944\n",
      "Epoch 27, Batch 1500, Loss: 0.3947\n",
      "Epoch 27, Batch 1600, Loss: 0.4119\n",
      "Epoch 27, Batch 1700, Loss: 0.3908\n",
      "Epoch 27, Batch 1800, Loss: 0.3994\n",
      "Epoch 27, Batch 1900, Loss: 0.3960\n",
      "Epoch 27, Batch 2000, Loss: 0.3953\n",
      "Epoch 27, Batch 2100, Loss: 0.3851\n",
      "Epoch 27, Batch 2200, Loss: 0.4128\n",
      "Epoch 27, Batch 2300, Loss: 0.4087\n",
      "Epoch 27, Batch 2400, Loss: 0.4038\n",
      "Epoch 27, Batch 2500, Loss: 0.4105\n",
      "Epoch 27, Batch 2600, Loss: 0.4142\n",
      "Epoch 27, Batch 2700, Loss: 0.3767\n",
      "Epoch 27, Batch 2800, Loss: 0.4067\n",
      "Epoch 27, Batch 2900, Loss: 0.4077\n",
      "Epoch 28, Batch 100, Loss: 0.3627\n",
      "Epoch 28, Batch 200, Loss: 0.3553\n",
      "Epoch 28, Batch 300, Loss: 0.3865\n",
      "Epoch 28, Batch 400, Loss: 0.3803\n",
      "Epoch 28, Batch 500, Loss: 0.3865\n",
      "Epoch 28, Batch 600, Loss: 0.3765\n",
      "Epoch 28, Batch 700, Loss: 0.3768\n",
      "Epoch 28, Batch 800, Loss: 0.3461\n",
      "Epoch 28, Batch 900, Loss: 0.3340\n",
      "Epoch 28, Batch 1000, Loss: 0.3632\n",
      "Epoch 28, Batch 1100, Loss: 0.3684\n",
      "Epoch 28, Batch 1200, Loss: 0.3716\n",
      "Epoch 28, Batch 1300, Loss: 0.3906\n",
      "Epoch 28, Batch 1400, Loss: 0.3822\n",
      "Epoch 28, Batch 1500, Loss: 0.3860\n",
      "Epoch 28, Batch 1600, Loss: 0.3708\n",
      "Epoch 28, Batch 1700, Loss: 0.3701\n",
      "Epoch 28, Batch 1800, Loss: 0.3704\n",
      "Epoch 28, Batch 1900, Loss: 0.4121\n",
      "Epoch 28, Batch 2000, Loss: 0.3690\n",
      "Epoch 28, Batch 2100, Loss: 0.3797\n",
      "Epoch 28, Batch 2200, Loss: 0.3759\n",
      "Epoch 28, Batch 2300, Loss: 0.4236\n",
      "Epoch 28, Batch 2400, Loss: 0.3761\n",
      "Epoch 28, Batch 2500, Loss: 0.3907\n",
      "Epoch 28, Batch 2600, Loss: 0.3923\n",
      "Epoch 28, Batch 2700, Loss: 0.3896\n",
      "Epoch 28, Batch 2800, Loss: 0.4037\n",
      "Epoch 28, Batch 2900, Loss: 0.3872\n",
      "Epoch 29, Batch 100, Loss: 0.3653\n",
      "Epoch 29, Batch 200, Loss: 0.3654\n",
      "Epoch 29, Batch 300, Loss: 0.3462\n",
      "Epoch 29, Batch 400, Loss: 0.3767\n",
      "Epoch 29, Batch 500, Loss: 0.3594\n",
      "Epoch 29, Batch 600, Loss: 0.3408\n",
      "Epoch 29, Batch 700, Loss: 0.3628\n",
      "Epoch 29, Batch 800, Loss: 0.3626\n",
      "Epoch 29, Batch 900, Loss: 0.3841\n",
      "Epoch 29, Batch 1000, Loss: 0.3505\n",
      "Epoch 29, Batch 1100, Loss: 0.3973\n",
      "Epoch 29, Batch 1200, Loss: 0.3834\n",
      "Epoch 29, Batch 1300, Loss: 0.3754\n",
      "Epoch 29, Batch 1400, Loss: 0.3704\n",
      "Epoch 29, Batch 1500, Loss: 0.3557\n",
      "Epoch 29, Batch 1600, Loss: 0.3630\n",
      "Epoch 29, Batch 1700, Loss: 0.3626\n",
      "Epoch 29, Batch 1800, Loss: 0.3607\n",
      "Epoch 29, Batch 1900, Loss: 0.3572\n",
      "Epoch 29, Batch 2000, Loss: 0.3644\n",
      "Epoch 29, Batch 2100, Loss: 0.3641\n",
      "Epoch 29, Batch 2200, Loss: 0.3928\n",
      "Epoch 29, Batch 2300, Loss: 0.3688\n",
      "Epoch 29, Batch 2400, Loss: 0.3711\n",
      "Epoch 29, Batch 2500, Loss: 0.3700\n",
      "Epoch 29, Batch 2600, Loss: 0.3783\n",
      "Epoch 29, Batch 2700, Loss: 0.3565\n",
      "Epoch 29, Batch 2800, Loss: 0.3587\n",
      "Epoch 29, Batch 2900, Loss: 0.3478\n",
      "Epoch 30, Batch 100, Loss: 0.3334\n",
      "Epoch 30, Batch 200, Loss: 0.3550\n",
      "Epoch 30, Batch 300, Loss: 0.3422\n",
      "Epoch 30, Batch 400, Loss: 0.3419\n",
      "Epoch 30, Batch 500, Loss: 0.3537\n",
      "Epoch 30, Batch 600, Loss: 0.3471\n",
      "Epoch 30, Batch 700, Loss: 0.3331\n",
      "Epoch 30, Batch 800, Loss: 0.3655\n",
      "Epoch 30, Batch 900, Loss: 0.3492\n",
      "Epoch 30, Batch 1000, Loss: 0.3395\n",
      "Epoch 30, Batch 1100, Loss: 0.3846\n",
      "Epoch 30, Batch 1200, Loss: 0.3452\n",
      "Epoch 30, Batch 1300, Loss: 0.3373\n",
      "Epoch 30, Batch 1400, Loss: 0.3571\n",
      "Epoch 30, Batch 1500, Loss: 0.3712\n",
      "Epoch 30, Batch 1600, Loss: 0.3605\n",
      "Epoch 30, Batch 1700, Loss: 0.3460\n",
      "Epoch 30, Batch 1800, Loss: 0.3494\n",
      "Epoch 30, Batch 1900, Loss: 0.3544\n",
      "Epoch 30, Batch 2000, Loss: 0.3544\n",
      "Epoch 30, Batch 2100, Loss: 0.3559\n",
      "Epoch 30, Batch 2200, Loss: 0.3817\n",
      "Epoch 30, Batch 2300, Loss: 0.3390\n",
      "Epoch 30, Batch 2400, Loss: 0.3550\n",
      "Epoch 30, Batch 2500, Loss: 0.3797\n",
      "Epoch 30, Batch 2600, Loss: 0.3430\n",
      "Epoch 30, Batch 2700, Loss: 0.3534\n",
      "Epoch 30, Batch 2800, Loss: 0.3636\n",
      "Epoch 30, Batch 2900, Loss: 0.3904\n",
      "Epoch 31, Batch 100, Loss: 0.3509\n",
      "Epoch 31, Batch 200, Loss: 0.3452\n",
      "Epoch 31, Batch 300, Loss: 0.3215\n",
      "Epoch 31, Batch 400, Loss: 0.3092\n",
      "Epoch 31, Batch 500, Loss: 0.3194\n",
      "Epoch 31, Batch 600, Loss: 0.3156\n",
      "Epoch 31, Batch 700, Loss: 0.3525\n",
      "Epoch 31, Batch 800, Loss: 0.3487\n",
      "Epoch 31, Batch 900, Loss: 0.3648\n",
      "Epoch 31, Batch 1000, Loss: 0.3369\n",
      "Epoch 31, Batch 1100, Loss: 0.3392\n",
      "Epoch 31, Batch 1200, Loss: 0.3194\n",
      "Epoch 31, Batch 1300, Loss: 0.3560\n",
      "Epoch 31, Batch 1400, Loss: 0.3441\n",
      "Epoch 31, Batch 1500, Loss: 0.3605\n",
      "Epoch 31, Batch 1600, Loss: 0.3311\n",
      "Epoch 31, Batch 1700, Loss: 0.3437\n",
      "Epoch 31, Batch 1800, Loss: 0.3480\n",
      "Epoch 31, Batch 1900, Loss: 0.3297\n",
      "Epoch 31, Batch 2000, Loss: 0.3796\n",
      "Epoch 31, Batch 2100, Loss: 0.3641\n",
      "Epoch 31, Batch 2200, Loss: 0.3560\n",
      "Epoch 31, Batch 2300, Loss: 0.3623\n",
      "Epoch 31, Batch 2400, Loss: 0.3477\n",
      "Epoch 31, Batch 2500, Loss: 0.3593\n",
      "Epoch 31, Batch 2600, Loss: 0.3634\n",
      "Epoch 31, Batch 2700, Loss: 0.3549\n",
      "Epoch 31, Batch 2800, Loss: 0.3349\n",
      "Epoch 31, Batch 2900, Loss: 0.3339\n",
      "Epoch 32, Batch 100, Loss: 0.3298\n",
      "Epoch 32, Batch 200, Loss: 0.3225\n",
      "Epoch 32, Batch 300, Loss: 0.3204\n",
      "Epoch 32, Batch 400, Loss: 0.3145\n",
      "Epoch 32, Batch 500, Loss: 0.3183\n",
      "Epoch 32, Batch 600, Loss: 0.3215\n",
      "Epoch 32, Batch 700, Loss: 0.3397\n",
      "Epoch 32, Batch 800, Loss: 0.3476\n",
      "Epoch 32, Batch 900, Loss: 0.3339\n",
      "Epoch 32, Batch 1000, Loss: 0.3200\n",
      "Epoch 32, Batch 1100, Loss: 0.3310\n",
      "Epoch 32, Batch 1200, Loss: 0.3417\n",
      "Epoch 32, Batch 1300, Loss: 0.3755\n",
      "Epoch 32, Batch 1400, Loss: 0.3633\n",
      "Epoch 32, Batch 1500, Loss: 0.3364\n",
      "Epoch 32, Batch 1600, Loss: 0.3350\n",
      "Epoch 32, Batch 1700, Loss: 0.3440\n",
      "Epoch 32, Batch 1800, Loss: 0.3239\n",
      "Epoch 32, Batch 1900, Loss: 0.3282\n",
      "Epoch 32, Batch 2000, Loss: 0.3399\n",
      "Epoch 32, Batch 2100, Loss: 0.3190\n",
      "Epoch 32, Batch 2200, Loss: 0.3292\n",
      "Epoch 32, Batch 2300, Loss: 0.3496\n",
      "Epoch 32, Batch 2400, Loss: 0.3448\n",
      "Epoch 32, Batch 2500, Loss: 0.3495\n",
      "Epoch 32, Batch 2600, Loss: 0.3272\n",
      "Epoch 32, Batch 2700, Loss: 0.3551\n",
      "Epoch 32, Batch 2800, Loss: 0.3307\n",
      "Epoch 32, Batch 2900, Loss: 0.3388\n",
      "Epoch 33, Batch 100, Loss: 0.3108\n",
      "Epoch 33, Batch 200, Loss: 0.3110\n",
      "Epoch 33, Batch 300, Loss: 0.3239\n",
      "Epoch 33, Batch 400, Loss: 0.3132\n",
      "Epoch 33, Batch 500, Loss: 0.2956\n",
      "Epoch 33, Batch 600, Loss: 0.3544\n",
      "Epoch 33, Batch 700, Loss: 0.3142\n",
      "Epoch 33, Batch 800, Loss: 0.3053\n",
      "Epoch 33, Batch 900, Loss: 0.3331\n",
      "Epoch 33, Batch 1000, Loss: 0.3212\n",
      "Epoch 33, Batch 1100, Loss: 0.3275\n",
      "Epoch 33, Batch 1200, Loss: 0.3374\n",
      "Epoch 33, Batch 1300, Loss: 0.3207\n",
      "Epoch 33, Batch 1400, Loss: 0.3328\n",
      "Epoch 33, Batch 1500, Loss: 0.3236\n",
      "Epoch 33, Batch 1600, Loss: 0.3449\n",
      "Epoch 33, Batch 1700, Loss: 0.3286\n",
      "Epoch 33, Batch 1800, Loss: 0.3462\n",
      "Epoch 33, Batch 1900, Loss: 0.3294\n",
      "Epoch 33, Batch 2000, Loss: 0.3235\n",
      "Epoch 33, Batch 2100, Loss: 0.3572\n",
      "Epoch 33, Batch 2200, Loss: 0.3363\n",
      "Epoch 33, Batch 2300, Loss: 0.3191\n",
      "Epoch 33, Batch 2400, Loss: 0.3277\n",
      "Epoch 33, Batch 2500, Loss: 0.3350\n",
      "Epoch 33, Batch 2600, Loss: 0.3340\n",
      "Epoch 33, Batch 2700, Loss: 0.3212\n",
      "Epoch 33, Batch 2800, Loss: 0.3253\n",
      "Epoch 33, Batch 2900, Loss: 0.3359\n",
      "Epoch 34, Batch 100, Loss: 0.3329\n",
      "Epoch 34, Batch 200, Loss: 0.3200\n",
      "Epoch 34, Batch 300, Loss: 0.3376\n",
      "Epoch 34, Batch 400, Loss: 0.2904\n",
      "Epoch 34, Batch 500, Loss: 0.3167\n",
      "Epoch 34, Batch 600, Loss: 0.3155\n",
      "Epoch 34, Batch 700, Loss: 0.3207\n",
      "Epoch 34, Batch 800, Loss: 0.3291\n",
      "Epoch 34, Batch 900, Loss: 0.3207\n",
      "Epoch 34, Batch 1000, Loss: 0.3289\n",
      "Epoch 34, Batch 1100, Loss: 0.3205\n",
      "Epoch 34, Batch 1200, Loss: 0.3042\n",
      "Epoch 34, Batch 1300, Loss: 0.3273\n",
      "Epoch 34, Batch 1400, Loss: 0.3209\n",
      "Epoch 34, Batch 1500, Loss: 0.2989\n",
      "Epoch 34, Batch 1600, Loss: 0.3135\n",
      "Epoch 34, Batch 1700, Loss: 0.3020\n",
      "Epoch 34, Batch 1800, Loss: 0.3208\n",
      "Epoch 34, Batch 1900, Loss: 0.3066\n",
      "Epoch 34, Batch 2000, Loss: 0.3477\n",
      "Epoch 34, Batch 2100, Loss: 0.3085\n",
      "Epoch 34, Batch 2200, Loss: 0.2983\n",
      "Epoch 34, Batch 2300, Loss: 0.3259\n",
      "Epoch 34, Batch 2400, Loss: 0.3394\n",
      "Epoch 34, Batch 2500, Loss: 0.3238\n",
      "Epoch 34, Batch 2600, Loss: 0.3395\n",
      "Epoch 34, Batch 2700, Loss: 0.3376\n",
      "Epoch 34, Batch 2800, Loss: 0.3175\n",
      "Epoch 34, Batch 2900, Loss: 0.3164\n",
      "Epoch 35, Batch 100, Loss: 0.3247\n",
      "Epoch 35, Batch 200, Loss: 0.3025\n",
      "Epoch 35, Batch 300, Loss: 0.3079\n",
      "Epoch 35, Batch 400, Loss: 0.2928\n",
      "Epoch 35, Batch 500, Loss: 0.2979\n",
      "Epoch 35, Batch 600, Loss: 0.3099\n",
      "Epoch 35, Batch 700, Loss: 0.2886\n",
      "Epoch 35, Batch 800, Loss: 0.3132\n",
      "Epoch 35, Batch 900, Loss: 0.3150\n",
      "Epoch 35, Batch 1000, Loss: 0.3097\n",
      "Epoch 35, Batch 1100, Loss: 0.2989\n",
      "Epoch 35, Batch 1200, Loss: 0.2945\n",
      "Epoch 35, Batch 1300, Loss: 0.3153\n",
      "Epoch 35, Batch 1400, Loss: 0.3095\n",
      "Epoch 35, Batch 1500, Loss: 0.2919\n",
      "Epoch 35, Batch 1600, Loss: 0.3146\n",
      "Epoch 35, Batch 1700, Loss: 0.3052\n",
      "Epoch 35, Batch 1800, Loss: 0.2961\n",
      "Epoch 35, Batch 1900, Loss: 0.2890\n",
      "Epoch 35, Batch 2000, Loss: 0.3082\n",
      "Epoch 35, Batch 2100, Loss: 0.3056\n",
      "Epoch 35, Batch 2200, Loss: 0.3210\n",
      "Epoch 35, Batch 2300, Loss: 0.3097\n",
      "Epoch 35, Batch 2400, Loss: 0.3314\n",
      "Epoch 35, Batch 2500, Loss: 0.3322\n",
      "Epoch 35, Batch 2600, Loss: 0.3262\n",
      "Epoch 35, Batch 2700, Loss: 0.3076\n",
      "Epoch 35, Batch 2800, Loss: 0.3349\n",
      "Epoch 35, Batch 2900, Loss: 0.3095\n",
      "Epoch 36, Batch 100, Loss: 0.3126\n",
      "Epoch 36, Batch 200, Loss: 0.2984\n",
      "Epoch 36, Batch 300, Loss: 0.2990\n",
      "Epoch 36, Batch 400, Loss: 0.3078\n",
      "Epoch 36, Batch 500, Loss: 0.2784\n",
      "Epoch 36, Batch 600, Loss: 0.3066\n",
      "Epoch 36, Batch 700, Loss: 0.3036\n",
      "Epoch 36, Batch 800, Loss: 0.3108\n",
      "Epoch 36, Batch 900, Loss: 0.3047\n",
      "Epoch 36, Batch 1000, Loss: 0.3000\n",
      "Epoch 36, Batch 1100, Loss: 0.2833\n",
      "Epoch 36, Batch 1200, Loss: 0.2837\n",
      "Epoch 36, Batch 1300, Loss: 0.3123\n",
      "Epoch 36, Batch 1400, Loss: 0.3063\n",
      "Epoch 36, Batch 1500, Loss: 0.3072\n",
      "Epoch 36, Batch 1600, Loss: 0.2863\n",
      "Epoch 36, Batch 1700, Loss: 0.3021\n",
      "Epoch 36, Batch 1800, Loss: 0.3203\n",
      "Epoch 36, Batch 1900, Loss: 0.3197\n",
      "Epoch 36, Batch 2000, Loss: 0.2781\n",
      "Epoch 36, Batch 2100, Loss: 0.3122\n",
      "Epoch 36, Batch 2200, Loss: 0.3018\n",
      "Epoch 36, Batch 2300, Loss: 0.2970\n",
      "Epoch 36, Batch 2400, Loss: 0.2999\n",
      "Epoch 36, Batch 2500, Loss: 0.2974\n",
      "Epoch 36, Batch 2600, Loss: 0.2977\n",
      "Epoch 36, Batch 2700, Loss: 0.3019\n",
      "Epoch 36, Batch 2800, Loss: 0.3127\n",
      "Epoch 36, Batch 2900, Loss: 0.3104\n",
      "Epoch 37, Batch 100, Loss: 0.3018\n",
      "Epoch 37, Batch 200, Loss: 0.2878\n",
      "Epoch 37, Batch 300, Loss: 0.2970\n",
      "Epoch 37, Batch 400, Loss: 0.3028\n",
      "Epoch 37, Batch 500, Loss: 0.2761\n",
      "Epoch 37, Batch 600, Loss: 0.2813\n",
      "Epoch 37, Batch 700, Loss: 0.2875\n",
      "Epoch 37, Batch 800, Loss: 0.3040\n",
      "Epoch 37, Batch 900, Loss: 0.2804\n",
      "Epoch 37, Batch 1000, Loss: 0.2824\n",
      "Epoch 37, Batch 1100, Loss: 0.2961\n",
      "Epoch 37, Batch 1200, Loss: 0.2869\n",
      "Epoch 37, Batch 1300, Loss: 0.2990\n",
      "Epoch 37, Batch 1400, Loss: 0.2911\n",
      "Epoch 37, Batch 1500, Loss: 0.2800\n",
      "Epoch 37, Batch 1600, Loss: 0.3047\n",
      "Epoch 37, Batch 1700, Loss: 0.3035\n",
      "Epoch 37, Batch 1800, Loss: 0.2930\n",
      "Epoch 37, Batch 1900, Loss: 0.3161\n",
      "Epoch 37, Batch 2000, Loss: 0.3003\n",
      "Epoch 37, Batch 2100, Loss: 0.3017\n",
      "Epoch 37, Batch 2200, Loss: 0.2994\n",
      "Epoch 37, Batch 2300, Loss: 0.3094\n",
      "Epoch 37, Batch 2400, Loss: 0.2844\n",
      "Epoch 37, Batch 2500, Loss: 0.3154\n",
      "Epoch 37, Batch 2600, Loss: 0.2878\n",
      "Epoch 37, Batch 2700, Loss: 0.3152\n",
      "Epoch 37, Batch 2800, Loss: 0.3084\n",
      "Epoch 37, Batch 2900, Loss: 0.3122\n",
      "Epoch 38, Batch 100, Loss: 0.2832\n",
      "Epoch 38, Batch 200, Loss: 0.2690\n",
      "Epoch 38, Batch 300, Loss: 0.2928\n",
      "Epoch 38, Batch 400, Loss: 0.2660\n",
      "Epoch 38, Batch 500, Loss: 0.2971\n",
      "Epoch 38, Batch 600, Loss: 0.2820\n",
      "Epoch 38, Batch 700, Loss: 0.2816\n",
      "Epoch 38, Batch 800, Loss: 0.2603\n",
      "Epoch 38, Batch 900, Loss: 0.2968\n",
      "Epoch 38, Batch 1000, Loss: 0.2989\n",
      "Epoch 38, Batch 1100, Loss: 0.2544\n",
      "Epoch 38, Batch 1200, Loss: 0.3074\n",
      "Epoch 38, Batch 1300, Loss: 0.2855\n",
      "Epoch 38, Batch 1400, Loss: 0.3079\n",
      "Epoch 38, Batch 1500, Loss: 0.2962\n",
      "Epoch 38, Batch 1600, Loss: 0.3049\n",
      "Epoch 38, Batch 1700, Loss: 0.2734\n",
      "Epoch 38, Batch 1800, Loss: 0.2982\n",
      "Epoch 38, Batch 1900, Loss: 0.2929\n",
      "Epoch 38, Batch 2000, Loss: 0.2967\n",
      "Epoch 38, Batch 2100, Loss: 0.2998\n",
      "Epoch 38, Batch 2200, Loss: 0.2856\n",
      "Epoch 38, Batch 2300, Loss: 0.2924\n",
      "Epoch 38, Batch 2400, Loss: 0.2949\n",
      "Epoch 38, Batch 2500, Loss: 0.3013\n",
      "Epoch 38, Batch 2600, Loss: 0.2824\n",
      "Epoch 38, Batch 2700, Loss: 0.3067\n",
      "Epoch 38, Batch 2800, Loss: 0.2805\n",
      "Epoch 38, Batch 2900, Loss: 0.2886\n",
      "Epoch 39, Batch 100, Loss: 0.2822\n",
      "Epoch 39, Batch 200, Loss: 0.2788\n",
      "Epoch 39, Batch 300, Loss: 0.2767\n",
      "Epoch 39, Batch 400, Loss: 0.2591\n",
      "Epoch 39, Batch 500, Loss: 0.2771\n",
      "Epoch 39, Batch 600, Loss: 0.2881\n",
      "Epoch 39, Batch 700, Loss: 0.2818\n",
      "Epoch 39, Batch 800, Loss: 0.2937\n",
      "Epoch 39, Batch 900, Loss: 0.2752\n",
      "Epoch 39, Batch 1000, Loss: 0.2771\n",
      "Epoch 39, Batch 1100, Loss: 0.3053\n",
      "Epoch 39, Batch 1200, Loss: 0.2975\n",
      "Epoch 39, Batch 1300, Loss: 0.2596\n",
      "Epoch 39, Batch 1400, Loss: 0.2730\n",
      "Epoch 39, Batch 1500, Loss: 0.2827\n",
      "Epoch 39, Batch 1600, Loss: 0.2870\n",
      "Epoch 39, Batch 1700, Loss: 0.2733\n",
      "Epoch 39, Batch 1800, Loss: 0.3071\n",
      "Epoch 39, Batch 1900, Loss: 0.2685\n",
      "Epoch 39, Batch 2000, Loss: 0.2843\n",
      "Epoch 39, Batch 2100, Loss: 0.2807\n",
      "Epoch 39, Batch 2200, Loss: 0.2909\n",
      "Epoch 39, Batch 2300, Loss: 0.2843\n",
      "Epoch 39, Batch 2400, Loss: 0.2577\n",
      "Epoch 39, Batch 2500, Loss: 0.2999\n",
      "Epoch 39, Batch 2600, Loss: 0.2765\n",
      "Epoch 39, Batch 2700, Loss: 0.2901\n",
      "Epoch 39, Batch 2800, Loss: 0.2816\n",
      "Epoch 39, Batch 2900, Loss: 0.2655\n",
      "Epoch 40, Batch 100, Loss: 0.2660\n",
      "Epoch 40, Batch 200, Loss: 0.2681\n",
      "Epoch 40, Batch 300, Loss: 0.2723\n",
      "Epoch 40, Batch 400, Loss: 0.2758\n",
      "Epoch 40, Batch 500, Loss: 0.2707\n",
      "Epoch 40, Batch 600, Loss: 0.2516\n",
      "Epoch 40, Batch 700, Loss: 0.2711\n",
      "Epoch 40, Batch 800, Loss: 0.2611\n",
      "Epoch 40, Batch 900, Loss: 0.2721\n",
      "Epoch 40, Batch 1000, Loss: 0.2704\n",
      "Epoch 40, Batch 1100, Loss: 0.2441\n",
      "Epoch 40, Batch 1200, Loss: 0.2504\n",
      "Epoch 40, Batch 1300, Loss: 0.2807\n",
      "Epoch 40, Batch 1400, Loss: 0.2694\n",
      "Epoch 40, Batch 1500, Loss: 0.2809\n",
      "Epoch 40, Batch 1600, Loss: 0.2616\n",
      "Epoch 40, Batch 1700, Loss: 0.2940\n",
      "Epoch 40, Batch 1800, Loss: 0.2942\n",
      "Epoch 40, Batch 1900, Loss: 0.2658\n",
      "Epoch 40, Batch 2000, Loss: 0.2808\n",
      "Epoch 40, Batch 2100, Loss: 0.2884\n",
      "Epoch 40, Batch 2200, Loss: 0.2538\n",
      "Epoch 40, Batch 2300, Loss: 0.2832\n",
      "Epoch 40, Batch 2400, Loss: 0.2710\n",
      "Epoch 40, Batch 2500, Loss: 0.2606\n",
      "Epoch 40, Batch 2600, Loss: 0.2708\n",
      "Epoch 40, Batch 2700, Loss: 0.2667\n",
      "Epoch 40, Batch 2800, Loss: 0.3008\n",
      "Epoch 40, Batch 2900, Loss: 0.2966\n",
      "Epoch 41, Batch 100, Loss: 0.2506\n",
      "Epoch 41, Batch 200, Loss: 0.2646\n",
      "Epoch 41, Batch 300, Loss: 0.2586\n",
      "Epoch 41, Batch 400, Loss: 0.2502\n",
      "Epoch 41, Batch 500, Loss: 0.2734\n",
      "Epoch 41, Batch 600, Loss: 0.2439\n",
      "Epoch 41, Batch 700, Loss: 0.2516\n",
      "Epoch 41, Batch 800, Loss: 0.2586\n",
      "Epoch 41, Batch 900, Loss: 0.2662\n",
      "Epoch 41, Batch 1000, Loss: 0.2770\n",
      "Epoch 41, Batch 1100, Loss: 0.2648\n",
      "Epoch 41, Batch 1200, Loss: 0.2658\n",
      "Epoch 41, Batch 1300, Loss: 0.2766\n",
      "Epoch 41, Batch 1400, Loss: 0.2624\n",
      "Epoch 41, Batch 1500, Loss: 0.2762\n",
      "Epoch 41, Batch 1600, Loss: 0.2791\n",
      "Epoch 41, Batch 1700, Loss: 0.2731\n",
      "Epoch 41, Batch 1800, Loss: 0.2663\n",
      "Epoch 41, Batch 1900, Loss: 0.2672\n",
      "Epoch 41, Batch 2000, Loss: 0.2608\n",
      "Epoch 41, Batch 2100, Loss: 0.2679\n",
      "Epoch 41, Batch 2200, Loss: 0.2577\n",
      "Epoch 41, Batch 2300, Loss: 0.2309\n",
      "Epoch 41, Batch 2400, Loss: 0.2986\n",
      "Epoch 41, Batch 2500, Loss: 0.2876\n",
      "Epoch 41, Batch 2600, Loss: 0.2713\n",
      "Epoch 41, Batch 2700, Loss: 0.2704\n",
      "Epoch 41, Batch 2800, Loss: 0.2649\n",
      "Epoch 41, Batch 2900, Loss: 0.2734\n",
      "Epoch 42, Batch 100, Loss: 0.2571\n",
      "Epoch 42, Batch 200, Loss: 0.2507\n",
      "Epoch 42, Batch 300, Loss: 0.2498\n",
      "Epoch 42, Batch 400, Loss: 0.2526\n",
      "Epoch 42, Batch 500, Loss: 0.2574\n",
      "Epoch 42, Batch 600, Loss: 0.2612\n",
      "Epoch 42, Batch 700, Loss: 0.2358\n",
      "Epoch 42, Batch 800, Loss: 0.2612\n",
      "Epoch 42, Batch 900, Loss: 0.2639\n",
      "Epoch 42, Batch 1000, Loss: 0.2599\n",
      "Epoch 42, Batch 1100, Loss: 0.2396\n",
      "Epoch 42, Batch 1200, Loss: 0.2544\n",
      "Epoch 42, Batch 1300, Loss: 0.2866\n",
      "Epoch 42, Batch 1400, Loss: 0.2665\n",
      "Epoch 42, Batch 1500, Loss: 0.2582\n",
      "Epoch 42, Batch 1600, Loss: 0.2541\n",
      "Epoch 42, Batch 1700, Loss: 0.2604\n",
      "Epoch 42, Batch 1800, Loss: 0.2515\n",
      "Epoch 42, Batch 1900, Loss: 0.2609\n",
      "Epoch 42, Batch 2000, Loss: 0.2636\n",
      "Epoch 42, Batch 2100, Loss: 0.2692\n",
      "Epoch 42, Batch 2200, Loss: 0.2787\n",
      "Epoch 42, Batch 2300, Loss: 0.2809\n",
      "Epoch 42, Batch 2400, Loss: 0.2593\n",
      "Epoch 42, Batch 2500, Loss: 0.2641\n",
      "Epoch 42, Batch 2600, Loss: 0.2809\n",
      "Epoch 42, Batch 2700, Loss: 0.2657\n",
      "Epoch 42, Batch 2800, Loss: 0.2624\n",
      "Epoch 42, Batch 2900, Loss: 0.2518\n",
      "Epoch 43, Batch 100, Loss: 0.2453\n",
      "Epoch 43, Batch 200, Loss: 0.2552\n",
      "Epoch 43, Batch 300, Loss: 0.2392\n",
      "Epoch 43, Batch 400, Loss: 0.2549\n",
      "Epoch 43, Batch 500, Loss: 0.2547\n",
      "Epoch 43, Batch 600, Loss: 0.2612\n",
      "Epoch 43, Batch 700, Loss: 0.2554\n",
      "Epoch 43, Batch 800, Loss: 0.2408\n",
      "Epoch 43, Batch 900, Loss: 0.2441\n",
      "Epoch 43, Batch 1000, Loss: 0.2401\n",
      "Epoch 43, Batch 1100, Loss: 0.2377\n",
      "Epoch 43, Batch 1200, Loss: 0.2410\n",
      "Epoch 43, Batch 1300, Loss: 0.2487\n",
      "Epoch 43, Batch 1400, Loss: 0.2572\n",
      "Epoch 43, Batch 1500, Loss: 0.2826\n",
      "Epoch 43, Batch 1600, Loss: 0.2748\n",
      "Epoch 43, Batch 1700, Loss: 0.2611\n",
      "Epoch 43, Batch 1800, Loss: 0.2667\n",
      "Epoch 43, Batch 1900, Loss: 0.2574\n",
      "Epoch 43, Batch 2000, Loss: 0.2513\n",
      "Epoch 43, Batch 2100, Loss: 0.2568\n",
      "Epoch 43, Batch 2200, Loss: 0.2494\n",
      "Epoch 43, Batch 2300, Loss: 0.2610\n",
      "Epoch 43, Batch 2400, Loss: 0.2712\n",
      "Epoch 43, Batch 2500, Loss: 0.2586\n",
      "Epoch 43, Batch 2600, Loss: 0.2498\n",
      "Epoch 43, Batch 2700, Loss: 0.2695\n",
      "Epoch 43, Batch 2800, Loss: 0.2558\n",
      "Epoch 43, Batch 2900, Loss: 0.2541\n",
      "Epoch 44, Batch 100, Loss: 0.2482\n",
      "Epoch 44, Batch 200, Loss: 0.2267\n",
      "Epoch 44, Batch 300, Loss: 0.2598\n",
      "Epoch 44, Batch 400, Loss: 0.2431\n",
      "Epoch 44, Batch 500, Loss: 0.2483\n",
      "Epoch 44, Batch 600, Loss: 0.2314\n",
      "Epoch 44, Batch 700, Loss: 0.2419\n",
      "Epoch 44, Batch 800, Loss: 0.2429\n",
      "Epoch 44, Batch 900, Loss: 0.2530\n",
      "Epoch 44, Batch 1000, Loss: 0.2630\n",
      "Epoch 44, Batch 1100, Loss: 0.2518\n",
      "Epoch 44, Batch 1200, Loss: 0.2284\n",
      "Epoch 44, Batch 1300, Loss: 0.2687\n",
      "Epoch 44, Batch 1400, Loss: 0.2393\n",
      "Epoch 44, Batch 1500, Loss: 0.2648\n",
      "Epoch 44, Batch 1600, Loss: 0.2671\n",
      "Epoch 44, Batch 1700, Loss: 0.2456\n",
      "Epoch 44, Batch 1800, Loss: 0.2536\n",
      "Epoch 44, Batch 1900, Loss: 0.2520\n",
      "Epoch 44, Batch 2000, Loss: 0.2410\n",
      "Epoch 44, Batch 2100, Loss: 0.2523\n",
      "Epoch 44, Batch 2200, Loss: 0.2671\n",
      "Epoch 44, Batch 2300, Loss: 0.2616\n",
      "Epoch 44, Batch 2400, Loss: 0.2716\n",
      "Epoch 44, Batch 2500, Loss: 0.2399\n",
      "Epoch 44, Batch 2600, Loss: 0.2470\n",
      "Epoch 44, Batch 2700, Loss: 0.2650\n",
      "Epoch 44, Batch 2800, Loss: 0.2728\n",
      "Epoch 44, Batch 2900, Loss: 0.2420\n",
      "Epoch 45, Batch 100, Loss: 0.2622\n",
      "Epoch 45, Batch 200, Loss: 0.2343\n",
      "Epoch 45, Batch 300, Loss: 0.2318\n",
      "Epoch 45, Batch 400, Loss: 0.2511\n",
      "Epoch 45, Batch 500, Loss: 0.2532\n",
      "Epoch 45, Batch 600, Loss: 0.2379\n",
      "Epoch 45, Batch 700, Loss: 0.2404\n",
      "Epoch 45, Batch 800, Loss: 0.2459\n",
      "Epoch 45, Batch 900, Loss: 0.2496\n",
      "Epoch 45, Batch 1000, Loss: 0.2340\n",
      "Epoch 45, Batch 1100, Loss: 0.2525\n",
      "Epoch 45, Batch 1200, Loss: 0.2451\n",
      "Epoch 45, Batch 1300, Loss: 0.2436\n",
      "Epoch 45, Batch 1400, Loss: 0.2539\n",
      "Epoch 45, Batch 1500, Loss: 0.2534\n",
      "Epoch 45, Batch 1600, Loss: 0.2435\n",
      "Epoch 45, Batch 1700, Loss: 0.2531\n",
      "Epoch 45, Batch 1800, Loss: 0.2543\n",
      "Epoch 45, Batch 1900, Loss: 0.2403\n",
      "Epoch 45, Batch 2000, Loss: 0.2523\n",
      "Epoch 45, Batch 2100, Loss: 0.2519\n",
      "Epoch 45, Batch 2200, Loss: 0.2608\n",
      "Epoch 45, Batch 2300, Loss: 0.2593\n",
      "Epoch 45, Batch 2400, Loss: 0.2546\n",
      "Epoch 45, Batch 2500, Loss: 0.2505\n",
      "Epoch 45, Batch 2600, Loss: 0.2263\n",
      "Epoch 45, Batch 2700, Loss: 0.2277\n",
      "Epoch 45, Batch 2800, Loss: 0.2247\n",
      "Epoch 45, Batch 2900, Loss: 0.2468\n",
      "Epoch 46, Batch 100, Loss: 0.2387\n",
      "Epoch 46, Batch 200, Loss: 0.2319\n",
      "Epoch 46, Batch 300, Loss: 0.2252\n",
      "Epoch 46, Batch 400, Loss: 0.2322\n",
      "Epoch 46, Batch 500, Loss: 0.2385\n",
      "Epoch 46, Batch 600, Loss: 0.2309\n",
      "Epoch 46, Batch 700, Loss: 0.2668\n",
      "Epoch 46, Batch 800, Loss: 0.2569\n",
      "Epoch 46, Batch 900, Loss: 0.2243\n",
      "Epoch 46, Batch 1000, Loss: 0.2483\n",
      "Epoch 46, Batch 1100, Loss: 0.2337\n",
      "Epoch 46, Batch 1200, Loss: 0.2471\n",
      "Epoch 46, Batch 1300, Loss: 0.2465\n",
      "Epoch 46, Batch 1400, Loss: 0.2317\n",
      "Epoch 46, Batch 1500, Loss: 0.2304\n",
      "Epoch 46, Batch 1600, Loss: 0.2489\n",
      "Epoch 46, Batch 1700, Loss: 0.2588\n",
      "Epoch 46, Batch 1800, Loss: 0.2249\n",
      "Epoch 46, Batch 1900, Loss: 0.2457\n",
      "Epoch 46, Batch 2000, Loss: 0.2495\n",
      "Epoch 46, Batch 2100, Loss: 0.2429\n",
      "Epoch 46, Batch 2200, Loss: 0.2381\n",
      "Epoch 46, Batch 2300, Loss: 0.2381\n",
      "Epoch 46, Batch 2400, Loss: 0.2361\n",
      "Epoch 46, Batch 2500, Loss: 0.2547\n",
      "Epoch 46, Batch 2600, Loss: 0.2335\n",
      "Epoch 46, Batch 2700, Loss: 0.2499\n",
      "Epoch 46, Batch 2800, Loss: 0.2473\n",
      "Epoch 46, Batch 2900, Loss: 0.2435\n",
      "Epoch 47, Batch 100, Loss: 0.2285\n",
      "Epoch 47, Batch 200, Loss: 0.2305\n",
      "Epoch 47, Batch 300, Loss: 0.2420\n",
      "Epoch 47, Batch 400, Loss: 0.2172\n",
      "Epoch 47, Batch 500, Loss: 0.2332\n",
      "Epoch 47, Batch 600, Loss: 0.2551\n",
      "Epoch 47, Batch 700, Loss: 0.2318\n",
      "Epoch 47, Batch 800, Loss: 0.2275\n",
      "Epoch 47, Batch 900, Loss: 0.2362\n",
      "Epoch 47, Batch 1000, Loss: 0.2436\n",
      "Epoch 47, Batch 1100, Loss: 0.2492\n",
      "Epoch 47, Batch 1200, Loss: 0.2414\n",
      "Epoch 47, Batch 1300, Loss: 0.2348\n",
      "Epoch 47, Batch 1400, Loss: 0.2523\n",
      "Epoch 47, Batch 1500, Loss: 0.2487\n",
      "Epoch 47, Batch 1600, Loss: 0.2382\n",
      "Epoch 47, Batch 1700, Loss: 0.2217\n",
      "Epoch 47, Batch 1800, Loss: 0.2336\n",
      "Epoch 47, Batch 1900, Loss: 0.2465\n",
      "Epoch 47, Batch 2000, Loss: 0.2264\n",
      "Epoch 47, Batch 2100, Loss: 0.2501\n",
      "Epoch 47, Batch 2200, Loss: 0.2440\n",
      "Epoch 47, Batch 2300, Loss: 0.2409\n",
      "Epoch 47, Batch 2400, Loss: 0.2362\n",
      "Epoch 47, Batch 2500, Loss: 0.2590\n",
      "Epoch 47, Batch 2600, Loss: 0.2246\n",
      "Epoch 47, Batch 2700, Loss: 0.2195\n",
      "Epoch 47, Batch 2800, Loss: 0.2386\n",
      "Epoch 47, Batch 2900, Loss: 0.2432\n",
      "Epoch 48, Batch 100, Loss: 0.2187\n",
      "Epoch 48, Batch 200, Loss: 0.2231\n",
      "Epoch 48, Batch 300, Loss: 0.2194\n",
      "Epoch 48, Batch 400, Loss: 0.2366\n",
      "Epoch 48, Batch 500, Loss: 0.2270\n",
      "Epoch 48, Batch 600, Loss: 0.2178\n",
      "Epoch 48, Batch 700, Loss: 0.2404\n",
      "Epoch 48, Batch 800, Loss: 0.2463\n",
      "Epoch 48, Batch 900, Loss: 0.2208\n",
      "Epoch 48, Batch 1000, Loss: 0.2208\n",
      "Epoch 48, Batch 1100, Loss: 0.2201\n",
      "Epoch 48, Batch 1200, Loss: 0.2117\n",
      "Epoch 48, Batch 1300, Loss: 0.2358\n",
      "Epoch 48, Batch 1400, Loss: 0.2341\n",
      "Epoch 48, Batch 1500, Loss: 0.2248\n",
      "Epoch 48, Batch 1600, Loss: 0.2300\n",
      "Epoch 48, Batch 1700, Loss: 0.2305\n",
      "Epoch 48, Batch 1800, Loss: 0.2193\n",
      "Epoch 48, Batch 1900, Loss: 0.2402\n",
      "Epoch 48, Batch 2000, Loss: 0.2455\n",
      "Epoch 48, Batch 2100, Loss: 0.2424\n",
      "Epoch 48, Batch 2200, Loss: 0.2288\n",
      "Epoch 48, Batch 2300, Loss: 0.2269\n",
      "Epoch 48, Batch 2400, Loss: 0.2403\n",
      "Epoch 48, Batch 2500, Loss: 0.2444\n",
      "Epoch 48, Batch 2600, Loss: 0.2435\n",
      "Epoch 48, Batch 2700, Loss: 0.2397\n",
      "Epoch 48, Batch 2800, Loss: 0.2495\n",
      "Epoch 48, Batch 2900, Loss: 0.2463\n",
      "Epoch 49, Batch 100, Loss: 0.2356\n",
      "Epoch 49, Batch 200, Loss: 0.2226\n",
      "Epoch 49, Batch 300, Loss: 0.2337\n",
      "Epoch 49, Batch 400, Loss: 0.2289\n",
      "Epoch 49, Batch 500, Loss: 0.2390\n",
      "Epoch 49, Batch 600, Loss: 0.2145\n",
      "Epoch 49, Batch 700, Loss: 0.2119\n",
      "Epoch 49, Batch 800, Loss: 0.2215\n",
      "Epoch 49, Batch 900, Loss: 0.2463\n",
      "Epoch 49, Batch 1000, Loss: 0.2381\n",
      "Epoch 49, Batch 1100, Loss: 0.2240\n",
      "Epoch 49, Batch 1200, Loss: 0.2340\n",
      "Epoch 49, Batch 1300, Loss: 0.2464\n",
      "Epoch 49, Batch 1400, Loss: 0.2295\n",
      "Epoch 49, Batch 1500, Loss: 0.2195\n",
      "Epoch 49, Batch 1600, Loss: 0.2229\n",
      "Epoch 49, Batch 1700, Loss: 0.2388\n",
      "Epoch 49, Batch 1800, Loss: 0.2300\n",
      "Epoch 49, Batch 1900, Loss: 0.2126\n",
      "Epoch 49, Batch 2000, Loss: 0.2269\n",
      "Epoch 49, Batch 2100, Loss: 0.2296\n",
      "Epoch 49, Batch 2200, Loss: 0.2224\n",
      "Epoch 49, Batch 2300, Loss: 0.2293\n",
      "Epoch 49, Batch 2400, Loss: 0.2413\n",
      "Epoch 49, Batch 2500, Loss: 0.2433\n",
      "Epoch 49, Batch 2600, Loss: 0.2180\n",
      "Epoch 49, Batch 2700, Loss: 0.2577\n",
      "Epoch 49, Batch 2800, Loss: 0.2356\n",
      "Epoch 49, Batch 2900, Loss: 0.2170\n",
      "Epoch 50, Batch 100, Loss: 0.2260\n",
      "Epoch 50, Batch 200, Loss: 0.2062\n",
      "Epoch 50, Batch 300, Loss: 0.2079\n",
      "Epoch 50, Batch 400, Loss: 0.2162\n",
      "Epoch 50, Batch 500, Loss: 0.2218\n",
      "Epoch 50, Batch 600, Loss: 0.2161\n",
      "Epoch 50, Batch 700, Loss: 0.2498\n",
      "Epoch 50, Batch 800, Loss: 0.2320\n",
      "Epoch 50, Batch 900, Loss: 0.2218\n",
      "Epoch 50, Batch 1000, Loss: 0.2133\n",
      "Epoch 50, Batch 1100, Loss: 0.2123\n",
      "Epoch 50, Batch 1200, Loss: 0.2226\n",
      "Epoch 50, Batch 1300, Loss: 0.2136\n",
      "Epoch 50, Batch 1400, Loss: 0.2312\n",
      "Epoch 50, Batch 1500, Loss: 0.2230\n",
      "Epoch 50, Batch 1600, Loss: 0.2342\n",
      "Epoch 50, Batch 1700, Loss: 0.2158\n",
      "Epoch 50, Batch 1800, Loss: 0.2295\n",
      "Epoch 50, Batch 1900, Loss: 0.2318\n",
      "Epoch 50, Batch 2000, Loss: 0.2416\n",
      "Epoch 50, Batch 2100, Loss: 0.2362\n",
      "Epoch 50, Batch 2200, Loss: 0.2198\n",
      "Epoch 50, Batch 2300, Loss: 0.2226\n",
      "Epoch 50, Batch 2400, Loss: 0.2272\n",
      "Epoch 50, Batch 2500, Loss: 0.2254\n",
      "Epoch 50, Batch 2600, Loss: 0.2377\n",
      "Epoch 50, Batch 2700, Loss: 0.2463\n",
      "Epoch 50, Batch 2800, Loss: 0.2160\n",
      "Epoch 50, Batch 2900, Loss: 0.2215\n",
      "Epoch 51, Batch 100, Loss: 0.2275\n",
      "Epoch 51, Batch 200, Loss: 0.2172\n",
      "Epoch 51, Batch 300, Loss: 0.2001\n",
      "Epoch 51, Batch 400, Loss: 0.2163\n",
      "Epoch 51, Batch 500, Loss: 0.2298\n",
      "Epoch 51, Batch 600, Loss: 0.2209\n",
      "Epoch 51, Batch 700, Loss: 0.2130\n",
      "Epoch 51, Batch 800, Loss: 0.2019\n",
      "Epoch 51, Batch 900, Loss: 0.2228\n",
      "Epoch 51, Batch 1000, Loss: 0.2279\n",
      "Epoch 51, Batch 1100, Loss: 0.2382\n",
      "Epoch 51, Batch 1200, Loss: 0.1925\n",
      "Epoch 51, Batch 1300, Loss: 0.1987\n",
      "Epoch 51, Batch 1400, Loss: 0.2246\n",
      "Epoch 51, Batch 1500, Loss: 0.2132\n",
      "Epoch 51, Batch 1600, Loss: 0.2200\n",
      "Epoch 51, Batch 1700, Loss: 0.2400\n",
      "Epoch 51, Batch 1800, Loss: 0.2291\n",
      "Epoch 51, Batch 1900, Loss: 0.2335\n",
      "Epoch 51, Batch 2000, Loss: 0.2334\n",
      "Epoch 51, Batch 2100, Loss: 0.2220\n",
      "Epoch 51, Batch 2200, Loss: 0.2143\n",
      "Epoch 51, Batch 2300, Loss: 0.2117\n",
      "Epoch 51, Batch 2400, Loss: 0.2224\n",
      "Epoch 51, Batch 2500, Loss: 0.2190\n",
      "Epoch 51, Batch 2600, Loss: 0.2208\n",
      "Epoch 51, Batch 2700, Loss: 0.2364\n",
      "Epoch 51, Batch 2800, Loss: 0.2296\n",
      "Epoch 51, Batch 2900, Loss: 0.2029\n",
      "Epoch 52, Batch 100, Loss: 0.2014\n",
      "Epoch 52, Batch 200, Loss: 0.2076\n",
      "Epoch 52, Batch 300, Loss: 0.2083\n",
      "Epoch 52, Batch 400, Loss: 0.2350\n",
      "Epoch 52, Batch 500, Loss: 0.2102\n",
      "Epoch 52, Batch 600, Loss: 0.2227\n",
      "Epoch 52, Batch 700, Loss: 0.2344\n",
      "Epoch 52, Batch 800, Loss: 0.2081\n",
      "Epoch 52, Batch 900, Loss: 0.2060\n",
      "Epoch 52, Batch 1000, Loss: 0.2306\n",
      "Epoch 52, Batch 1100, Loss: 0.2193\n",
      "Epoch 52, Batch 1200, Loss: 0.2102\n",
      "Epoch 52, Batch 1300, Loss: 0.2228\n",
      "Epoch 52, Batch 1400, Loss: 0.2124\n",
      "Epoch 52, Batch 1500, Loss: 0.2200\n",
      "Epoch 52, Batch 1600, Loss: 0.2337\n",
      "Epoch 52, Batch 1700, Loss: 0.2277\n",
      "Epoch 52, Batch 1800, Loss: 0.2180\n",
      "Epoch 52, Batch 1900, Loss: 0.2093\n",
      "Epoch 52, Batch 2000, Loss: 0.2204\n",
      "Epoch 52, Batch 2100, Loss: 0.2231\n",
      "Epoch 52, Batch 2200, Loss: 0.2061\n",
      "Epoch 52, Batch 2300, Loss: 0.2091\n",
      "Epoch 52, Batch 2400, Loss: 0.2259\n",
      "Epoch 52, Batch 2500, Loss: 0.2172\n",
      "Epoch 52, Batch 2600, Loss: 0.2337\n",
      "Epoch 52, Batch 2700, Loss: 0.2119\n",
      "Epoch 52, Batch 2800, Loss: 0.2210\n",
      "Epoch 52, Batch 2900, Loss: 0.2046\n",
      "Epoch 53, Batch 100, Loss: 0.2094\n",
      "Epoch 53, Batch 200, Loss: 0.2001\n",
      "Epoch 53, Batch 300, Loss: 0.2065\n",
      "Epoch 53, Batch 400, Loss: 0.2221\n",
      "Epoch 53, Batch 500, Loss: 0.2107\n",
      "Epoch 53, Batch 600, Loss: 0.2108\n",
      "Epoch 53, Batch 700, Loss: 0.2318\n",
      "Epoch 53, Batch 800, Loss: 0.2243\n",
      "Epoch 53, Batch 900, Loss: 0.2051\n",
      "Epoch 53, Batch 1000, Loss: 0.2250\n",
      "Epoch 53, Batch 1100, Loss: 0.2096\n",
      "Epoch 53, Batch 1200, Loss: 0.1966\n",
      "Epoch 53, Batch 1300, Loss: 0.2128\n",
      "Epoch 53, Batch 1400, Loss: 0.2067\n",
      "Epoch 53, Batch 1500, Loss: 0.2291\n",
      "Epoch 53, Batch 1600, Loss: 0.2152\n",
      "Epoch 53, Batch 1700, Loss: 0.2085\n",
      "Epoch 53, Batch 1800, Loss: 0.2163\n",
      "Epoch 53, Batch 1900, Loss: 0.2041\n",
      "Epoch 53, Batch 2000, Loss: 0.2083\n",
      "Epoch 53, Batch 2100, Loss: 0.2331\n",
      "Epoch 53, Batch 2200, Loss: 0.2016\n",
      "Epoch 53, Batch 2300, Loss: 0.2200\n",
      "Epoch 53, Batch 2400, Loss: 0.2097\n",
      "Epoch 53, Batch 2500, Loss: 0.2161\n",
      "Epoch 53, Batch 2600, Loss: 0.2076\n",
      "Epoch 53, Batch 2700, Loss: 0.2247\n",
      "Epoch 53, Batch 2800, Loss: 0.2419\n",
      "Epoch 53, Batch 2900, Loss: 0.2179\n",
      "Epoch 54, Batch 100, Loss: 0.1949\n",
      "Epoch 54, Batch 200, Loss: 0.2133\n",
      "Epoch 54, Batch 300, Loss: 0.2132\n",
      "Epoch 54, Batch 400, Loss: 0.2103\n",
      "Epoch 54, Batch 500, Loss: 0.2043\n",
      "Epoch 54, Batch 600, Loss: 0.2116\n",
      "Epoch 54, Batch 700, Loss: 0.2035\n",
      "Epoch 54, Batch 800, Loss: 0.2116\n",
      "Epoch 54, Batch 900, Loss: 0.2108\n",
      "Epoch 54, Batch 1000, Loss: 0.2109\n",
      "Epoch 54, Batch 1100, Loss: 0.2118\n",
      "Epoch 54, Batch 1200, Loss: 0.2089\n",
      "Epoch 54, Batch 1300, Loss: 0.2010\n",
      "Epoch 54, Batch 1400, Loss: 0.2013\n",
      "Epoch 54, Batch 1500, Loss: 0.2156\n",
      "Epoch 54, Batch 1600, Loss: 0.2258\n",
      "Epoch 54, Batch 1700, Loss: 0.2053\n",
      "Epoch 54, Batch 1800, Loss: 0.2098\n",
      "Epoch 54, Batch 1900, Loss: 0.2274\n",
      "Epoch 54, Batch 2000, Loss: 0.2160\n",
      "Epoch 54, Batch 2100, Loss: 0.2161\n",
      "Epoch 54, Batch 2200, Loss: 0.2219\n",
      "Epoch 54, Batch 2300, Loss: 0.2090\n",
      "Epoch 54, Batch 2400, Loss: 0.2381\n",
      "Epoch 54, Batch 2500, Loss: 0.2298\n",
      "Epoch 54, Batch 2600, Loss: 0.2100\n",
      "Epoch 54, Batch 2700, Loss: 0.1986\n",
      "Epoch 54, Batch 2800, Loss: 0.2195\n",
      "Epoch 54, Batch 2900, Loss: 0.2069\n",
      "Epoch 55, Batch 100, Loss: 0.2038\n",
      "Epoch 55, Batch 200, Loss: 0.1981\n",
      "Epoch 55, Batch 300, Loss: 0.1976\n",
      "Epoch 55, Batch 400, Loss: 0.2028\n",
      "Epoch 55, Batch 500, Loss: 0.2049\n",
      "Epoch 55, Batch 600, Loss: 0.1956\n",
      "Epoch 55, Batch 700, Loss: 0.1953\n",
      "Epoch 55, Batch 800, Loss: 0.1935\n",
      "Epoch 55, Batch 900, Loss: 0.2082\n",
      "Epoch 55, Batch 1000, Loss: 0.2035\n",
      "Epoch 55, Batch 1100, Loss: 0.2237\n",
      "Epoch 55, Batch 1200, Loss: 0.2006\n",
      "Epoch 55, Batch 1300, Loss: 0.2109\n",
      "Epoch 55, Batch 1400, Loss: 0.2153\n",
      "Epoch 55, Batch 1500, Loss: 0.2084\n",
      "Epoch 55, Batch 1600, Loss: 0.2057\n",
      "Epoch 55, Batch 1700, Loss: 0.2181\n",
      "Epoch 55, Batch 1800, Loss: 0.1915\n",
      "Epoch 55, Batch 1900, Loss: 0.2117\n",
      "Epoch 55, Batch 2000, Loss: 0.2092\n",
      "Epoch 55, Batch 2100, Loss: 0.2330\n",
      "Epoch 55, Batch 2200, Loss: 0.2073\n",
      "Epoch 55, Batch 2300, Loss: 0.2038\n",
      "Epoch 55, Batch 2400, Loss: 0.2263\n",
      "Epoch 55, Batch 2500, Loss: 0.2150\n",
      "Epoch 55, Batch 2600, Loss: 0.2091\n",
      "Epoch 55, Batch 2700, Loss: 0.2111\n",
      "Epoch 55, Batch 2800, Loss: 0.2194\n",
      "Epoch 55, Batch 2900, Loss: 0.2211\n",
      "Epoch 56, Batch 100, Loss: 0.2101\n",
      "Epoch 56, Batch 200, Loss: 0.1928\n",
      "Epoch 56, Batch 300, Loss: 0.2047\n",
      "Epoch 56, Batch 400, Loss: 0.2044\n",
      "Epoch 56, Batch 500, Loss: 0.2090\n",
      "Epoch 56, Batch 600, Loss: 0.1925\n",
      "Epoch 56, Batch 700, Loss: 0.1870\n",
      "Epoch 56, Batch 800, Loss: 0.2002\n",
      "Epoch 56, Batch 900, Loss: 0.2046\n",
      "Epoch 56, Batch 1000, Loss: 0.2075\n",
      "Epoch 56, Batch 1100, Loss: 0.1988\n",
      "Epoch 56, Batch 1200, Loss: 0.2137\n",
      "Epoch 56, Batch 1300, Loss: 0.2004\n",
      "Epoch 56, Batch 1400, Loss: 0.2227\n",
      "Epoch 56, Batch 1500, Loss: 0.2055\n",
      "Epoch 56, Batch 1600, Loss: 0.2083\n",
      "Epoch 56, Batch 1700, Loss: 0.2016\n",
      "Epoch 56, Batch 1800, Loss: 0.2140\n",
      "Epoch 56, Batch 1900, Loss: 0.2121\n",
      "Epoch 56, Batch 2000, Loss: 0.2203\n",
      "Epoch 56, Batch 2100, Loss: 0.2035\n",
      "Epoch 56, Batch 2200, Loss: 0.2172\n",
      "Epoch 56, Batch 2300, Loss: 0.2088\n",
      "Epoch 56, Batch 2400, Loss: 0.2210\n",
      "Epoch 56, Batch 2500, Loss: 0.2171\n",
      "Epoch 56, Batch 2600, Loss: 0.1938\n",
      "Epoch 56, Batch 2700, Loss: 0.2036\n",
      "Epoch 56, Batch 2800, Loss: 0.2002\n",
      "Epoch 56, Batch 2900, Loss: 0.2061\n",
      "Epoch 57, Batch 100, Loss: 0.1789\n",
      "Epoch 57, Batch 200, Loss: 0.1996\n",
      "Epoch 57, Batch 300, Loss: 0.1895\n",
      "Epoch 57, Batch 400, Loss: 0.2027\n",
      "Epoch 57, Batch 500, Loss: 0.1993\n",
      "Epoch 57, Batch 600, Loss: 0.1865\n",
      "Epoch 57, Batch 700, Loss: 0.2088\n",
      "Epoch 57, Batch 800, Loss: 0.1996\n",
      "Epoch 57, Batch 900, Loss: 0.1985\n",
      "Epoch 57, Batch 1000, Loss: 0.1959\n",
      "Epoch 57, Batch 1100, Loss: 0.1969\n",
      "Epoch 57, Batch 1200, Loss: 0.2162\n",
      "Epoch 57, Batch 1300, Loss: 0.1863\n",
      "Epoch 57, Batch 1400, Loss: 0.1997\n",
      "Epoch 57, Batch 1500, Loss: 0.2031\n",
      "Epoch 57, Batch 1600, Loss: 0.1960\n",
      "Epoch 57, Batch 1700, Loss: 0.2018\n",
      "Epoch 57, Batch 1800, Loss: 0.2011\n",
      "Epoch 57, Batch 1900, Loss: 0.2094\n",
      "Epoch 57, Batch 2000, Loss: 0.2013\n",
      "Epoch 57, Batch 2100, Loss: 0.2106\n",
      "Epoch 57, Batch 2200, Loss: 0.2075\n",
      "Epoch 57, Batch 2300, Loss: 0.1961\n",
      "Epoch 57, Batch 2400, Loss: 0.2005\n",
      "Epoch 57, Batch 2500, Loss: 0.2173\n",
      "Epoch 57, Batch 2600, Loss: 0.2061\n",
      "Epoch 57, Batch 2700, Loss: 0.2126\n",
      "Epoch 57, Batch 2800, Loss: 0.2045\n",
      "Epoch 57, Batch 2900, Loss: 0.2134\n",
      "Epoch 58, Batch 100, Loss: 0.1972\n",
      "Epoch 58, Batch 200, Loss: 0.1909\n",
      "Epoch 58, Batch 300, Loss: 0.1989\n",
      "Epoch 58, Batch 400, Loss: 0.2102\n",
      "Epoch 58, Batch 500, Loss: 0.1938\n",
      "Epoch 58, Batch 600, Loss: 0.2059\n",
      "Epoch 58, Batch 700, Loss: 0.1771\n",
      "Epoch 58, Batch 800, Loss: 0.1920\n",
      "Epoch 58, Batch 900, Loss: 0.1932\n",
      "Epoch 58, Batch 1000, Loss: 0.1947\n",
      "Epoch 58, Batch 1100, Loss: 0.2017\n",
      "Epoch 58, Batch 1200, Loss: 0.1839\n",
      "Epoch 58, Batch 1300, Loss: 0.1852\n",
      "Epoch 58, Batch 1400, Loss: 0.1879\n",
      "Epoch 58, Batch 1500, Loss: 0.2023\n",
      "Epoch 58, Batch 1600, Loss: 0.2035\n",
      "Epoch 58, Batch 1700, Loss: 0.1972\n",
      "Epoch 58, Batch 1800, Loss: 0.2101\n",
      "Epoch 58, Batch 1900, Loss: 0.2212\n",
      "Epoch 58, Batch 2000, Loss: 0.2026\n",
      "Epoch 58, Batch 2100, Loss: 0.1944\n",
      "Epoch 58, Batch 2200, Loss: 0.2123\n",
      "Epoch 58, Batch 2300, Loss: 0.2145\n",
      "Epoch 58, Batch 2400, Loss: 0.2133\n",
      "Epoch 58, Batch 2500, Loss: 0.2003\n",
      "Epoch 58, Batch 2600, Loss: 0.2013\n",
      "Epoch 58, Batch 2700, Loss: 0.1861\n",
      "Epoch 58, Batch 2800, Loss: 0.1867\n",
      "Epoch 58, Batch 2900, Loss: 0.2136\n",
      "Epoch 59, Batch 100, Loss: 0.1910\n",
      "Epoch 59, Batch 200, Loss: 0.1961\n",
      "Epoch 59, Batch 300, Loss: 0.1762\n",
      "Epoch 59, Batch 400, Loss: 0.1975\n",
      "Epoch 59, Batch 500, Loss: 0.1719\n",
      "Epoch 59, Batch 600, Loss: 0.1908\n",
      "Epoch 59, Batch 700, Loss: 0.2148\n",
      "Epoch 59, Batch 800, Loss: 0.1932\n",
      "Epoch 59, Batch 900, Loss: 0.1944\n",
      "Epoch 59, Batch 1000, Loss: 0.2171\n",
      "Epoch 59, Batch 1100, Loss: 0.1996\n",
      "Epoch 59, Batch 1200, Loss: 0.2074\n",
      "Epoch 59, Batch 1300, Loss: 0.1898\n",
      "Epoch 59, Batch 1400, Loss: 0.1921\n",
      "Epoch 59, Batch 1500, Loss: 0.2053\n",
      "Epoch 59, Batch 1600, Loss: 0.1822\n",
      "Epoch 59, Batch 1700, Loss: 0.1994\n",
      "Epoch 59, Batch 1800, Loss: 0.1982\n",
      "Epoch 59, Batch 1900, Loss: 0.1870\n",
      "Epoch 59, Batch 2000, Loss: 0.2005\n",
      "Epoch 59, Batch 2100, Loss: 0.2003\n",
      "Epoch 59, Batch 2200, Loss: 0.2057\n",
      "Epoch 59, Batch 2300, Loss: 0.1879\n",
      "Epoch 59, Batch 2400, Loss: 0.1943\n",
      "Epoch 59, Batch 2500, Loss: 0.2040\n",
      "Epoch 59, Batch 2600, Loss: 0.2012\n",
      "Epoch 59, Batch 2700, Loss: 0.2059\n",
      "Epoch 59, Batch 2800, Loss: 0.2016\n",
      "Epoch 59, Batch 2900, Loss: 0.2002\n",
      "Epoch 60, Batch 100, Loss: 0.1826\n",
      "Epoch 60, Batch 200, Loss: 0.1898\n",
      "Epoch 60, Batch 300, Loss: 0.1899\n",
      "Epoch 60, Batch 400, Loss: 0.1758\n",
      "Epoch 60, Batch 500, Loss: 0.1895\n",
      "Epoch 60, Batch 600, Loss: 0.1865\n",
      "Epoch 60, Batch 700, Loss: 0.2022\n",
      "Epoch 60, Batch 800, Loss: 0.1899\n",
      "Epoch 60, Batch 900, Loss: 0.1945\n",
      "Epoch 60, Batch 1000, Loss: 0.1856\n",
      "Epoch 60, Batch 1100, Loss: 0.1957\n",
      "Epoch 60, Batch 1200, Loss: 0.1998\n",
      "Epoch 60, Batch 1300, Loss: 0.2027\n",
      "Epoch 60, Batch 1400, Loss: 0.1851\n",
      "Epoch 60, Batch 1500, Loss: 0.2160\n",
      "Epoch 60, Batch 1600, Loss: 0.2094\n",
      "Epoch 60, Batch 1700, Loss: 0.2057\n",
      "Epoch 60, Batch 1800, Loss: 0.1940\n",
      "Epoch 60, Batch 1900, Loss: 0.1923\n",
      "Epoch 60, Batch 2000, Loss: 0.2056\n",
      "Epoch 60, Batch 2100, Loss: 0.1953\n",
      "Epoch 60, Batch 2200, Loss: 0.2063\n",
      "Epoch 60, Batch 2300, Loss: 0.2037\n",
      "Epoch 60, Batch 2400, Loss: 0.1855\n",
      "Epoch 60, Batch 2500, Loss: 0.1985\n",
      "Epoch 60, Batch 2600, Loss: 0.1894\n",
      "Epoch 60, Batch 2700, Loss: 0.1973\n",
      "Epoch 60, Batch 2800, Loss: 0.1862\n",
      "Epoch 60, Batch 2900, Loss: 0.2056\n",
      "Epoch 61, Batch 100, Loss: 0.1968\n",
      "Epoch 61, Batch 200, Loss: 0.1833\n",
      "Epoch 61, Batch 300, Loss: 0.1971\n",
      "Epoch 61, Batch 400, Loss: 0.1838\n",
      "Epoch 61, Batch 500, Loss: 0.1776\n",
      "Epoch 61, Batch 600, Loss: 0.1848\n",
      "Epoch 61, Batch 700, Loss: 0.1968\n",
      "Epoch 61, Batch 800, Loss: 0.1838\n",
      "Epoch 61, Batch 900, Loss: 0.2044\n",
      "Epoch 61, Batch 1000, Loss: 0.1876\n",
      "Epoch 61, Batch 1100, Loss: 0.1848\n",
      "Epoch 61, Batch 1200, Loss: 0.1998\n",
      "Epoch 61, Batch 1300, Loss: 0.1913\n",
      "Epoch 61, Batch 1400, Loss: 0.1866\n",
      "Epoch 61, Batch 1500, Loss: 0.2019\n",
      "Epoch 61, Batch 1600, Loss: 0.1711\n",
      "Epoch 61, Batch 1700, Loss: 0.1872\n",
      "Epoch 61, Batch 1800, Loss: 0.1908\n",
      "Epoch 61, Batch 1900, Loss: 0.1943\n",
      "Epoch 61, Batch 2000, Loss: 0.1994\n",
      "Epoch 61, Batch 2100, Loss: 0.1857\n",
      "Epoch 61, Batch 2200, Loss: 0.2069\n",
      "Epoch 61, Batch 2300, Loss: 0.2193\n",
      "Epoch 61, Batch 2400, Loss: 0.1962\n",
      "Epoch 61, Batch 2500, Loss: 0.2019\n",
      "Epoch 61, Batch 2600, Loss: 0.1796\n",
      "Epoch 61, Batch 2700, Loss: 0.1856\n",
      "Epoch 61, Batch 2800, Loss: 0.1928\n",
      "Epoch 61, Batch 2900, Loss: 0.2029\n",
      "Epoch 62, Batch 100, Loss: 0.1749\n",
      "Epoch 62, Batch 200, Loss: 0.1918\n",
      "Epoch 62, Batch 300, Loss: 0.1882\n",
      "Epoch 62, Batch 400, Loss: 0.1834\n",
      "Epoch 62, Batch 500, Loss: 0.1829\n",
      "Epoch 62, Batch 600, Loss: 0.1965\n",
      "Epoch 62, Batch 700, Loss: 0.1846\n",
      "Epoch 62, Batch 800, Loss: 0.1978\n",
      "Epoch 62, Batch 900, Loss: 0.2004\n",
      "Epoch 62, Batch 1000, Loss: 0.1877\n",
      "Epoch 62, Batch 1100, Loss: 0.1888\n",
      "Epoch 62, Batch 1200, Loss: 0.1847\n",
      "Epoch 62, Batch 1300, Loss: 0.1650\n",
      "Epoch 62, Batch 1400, Loss: 0.1893\n",
      "Epoch 62, Batch 1500, Loss: 0.1960\n",
      "Epoch 62, Batch 1600, Loss: 0.1812\n",
      "Epoch 62, Batch 1700, Loss: 0.1924\n",
      "Epoch 62, Batch 1800, Loss: 0.1978\n",
      "Epoch 62, Batch 1900, Loss: 0.2123\n",
      "Epoch 62, Batch 2000, Loss: 0.2102\n",
      "Epoch 62, Batch 2100, Loss: 0.1798\n",
      "Epoch 62, Batch 2200, Loss: 0.1848\n",
      "Epoch 62, Batch 2300, Loss: 0.1950\n",
      "Epoch 62, Batch 2400, Loss: 0.1914\n",
      "Epoch 62, Batch 2500, Loss: 0.1964\n",
      "Epoch 62, Batch 2600, Loss: 0.1915\n",
      "Epoch 62, Batch 2700, Loss: 0.1850\n",
      "Epoch 62, Batch 2800, Loss: 0.2078\n",
      "Epoch 62, Batch 2900, Loss: 0.1816\n",
      "Epoch 63, Batch 100, Loss: 0.1666\n",
      "Epoch 63, Batch 200, Loss: 0.1832\n",
      "Epoch 63, Batch 300, Loss: 0.1846\n",
      "Epoch 63, Batch 400, Loss: 0.1751\n",
      "Epoch 63, Batch 500, Loss: 0.2014\n",
      "Epoch 63, Batch 600, Loss: 0.1835\n",
      "Epoch 63, Batch 700, Loss: 0.1922\n",
      "Epoch 63, Batch 800, Loss: 0.1917\n",
      "Epoch 63, Batch 900, Loss: 0.1805\n",
      "Epoch 63, Batch 1000, Loss: 0.1904\n",
      "Epoch 63, Batch 1100, Loss: 0.1788\n",
      "Epoch 63, Batch 1200, Loss: 0.1950\n",
      "Epoch 63, Batch 1300, Loss: 0.1733\n",
      "Epoch 63, Batch 1400, Loss: 0.1808\n",
      "Epoch 63, Batch 1500, Loss: 0.1837\n",
      "Epoch 63, Batch 1600, Loss: 0.1828\n",
      "Epoch 63, Batch 1700, Loss: 0.1966\n",
      "Epoch 63, Batch 1800, Loss: 0.1824\n",
      "Epoch 63, Batch 1900, Loss: 0.1912\n",
      "Epoch 63, Batch 2000, Loss: 0.1796\n",
      "Epoch 63, Batch 2100, Loss: 0.1973\n",
      "Epoch 63, Batch 2200, Loss: 0.1962\n",
      "Epoch 63, Batch 2300, Loss: 0.1965\n",
      "Epoch 63, Batch 2400, Loss: 0.1913\n",
      "Epoch 63, Batch 2500, Loss: 0.2051\n",
      "Epoch 63, Batch 2600, Loss: 0.1984\n",
      "Epoch 63, Batch 2700, Loss: 0.1960\n",
      "Epoch 63, Batch 2800, Loss: 0.1742\n",
      "Epoch 63, Batch 2900, Loss: 0.2006\n",
      "Epoch 64, Batch 100, Loss: 0.1758\n",
      "Epoch 64, Batch 200, Loss: 0.1846\n",
      "Epoch 64, Batch 300, Loss: 0.1875\n",
      "Epoch 64, Batch 400, Loss: 0.1809\n",
      "Epoch 64, Batch 500, Loss: 0.1741\n",
      "Epoch 64, Batch 600, Loss: 0.1903\n",
      "Epoch 64, Batch 700, Loss: 0.1732\n",
      "Epoch 64, Batch 800, Loss: 0.1824\n",
      "Epoch 64, Batch 900, Loss: 0.1711\n",
      "Epoch 64, Batch 1000, Loss: 0.1827\n",
      "Epoch 64, Batch 1100, Loss: 0.1777\n",
      "Epoch 64, Batch 1200, Loss: 0.1999\n",
      "Epoch 64, Batch 1300, Loss: 0.1942\n",
      "Epoch 64, Batch 1400, Loss: 0.1834\n",
      "Epoch 64, Batch 1500, Loss: 0.1847\n",
      "Epoch 64, Batch 1600, Loss: 0.1937\n",
      "Epoch 64, Batch 1700, Loss: 0.1852\n",
      "Epoch 64, Batch 1800, Loss: 0.1915\n",
      "Epoch 64, Batch 1900, Loss: 0.1783\n",
      "Epoch 64, Batch 2000, Loss: 0.1906\n",
      "Epoch 64, Batch 2100, Loss: 0.2014\n",
      "Epoch 64, Batch 2200, Loss: 0.1740\n",
      "Epoch 64, Batch 2300, Loss: 0.1996\n",
      "Epoch 64, Batch 2400, Loss: 0.1758\n",
      "Epoch 64, Batch 2500, Loss: 0.1967\n",
      "Epoch 64, Batch 2600, Loss: 0.1779\n",
      "Epoch 64, Batch 2700, Loss: 0.1896\n",
      "Epoch 64, Batch 2800, Loss: 0.1871\n",
      "Epoch 64, Batch 2900, Loss: 0.1985\n",
      "Epoch 65, Batch 100, Loss: 0.1850\n",
      "Epoch 65, Batch 200, Loss: 0.1808\n",
      "Epoch 65, Batch 300, Loss: 0.1733\n",
      "Epoch 65, Batch 400, Loss: 0.1588\n",
      "Epoch 65, Batch 500, Loss: 0.1789\n",
      "Epoch 65, Batch 600, Loss: 0.1761\n",
      "Epoch 65, Batch 700, Loss: 0.1782\n",
      "Epoch 65, Batch 800, Loss: 0.1768\n",
      "Epoch 65, Batch 900, Loss: 0.1955\n",
      "Epoch 65, Batch 1000, Loss: 0.1796\n",
      "Epoch 65, Batch 1100, Loss: 0.1818\n",
      "Epoch 65, Batch 1200, Loss: 0.1671\n",
      "Epoch 65, Batch 1300, Loss: 0.1914\n",
      "Epoch 65, Batch 1400, Loss: 0.1878\n",
      "Epoch 65, Batch 1500, Loss: 0.1794\n",
      "Epoch 65, Batch 1600, Loss: 0.1824\n",
      "Epoch 65, Batch 1700, Loss: 0.1991\n",
      "Epoch 65, Batch 1800, Loss: 0.1833\n",
      "Epoch 65, Batch 1900, Loss: 0.1787\n",
      "Epoch 65, Batch 2000, Loss: 0.1771\n",
      "Epoch 65, Batch 2100, Loss: 0.1713\n",
      "Epoch 65, Batch 2200, Loss: 0.1755\n",
      "Epoch 65, Batch 2300, Loss: 0.1867\n",
      "Epoch 65, Batch 2400, Loss: 0.1901\n",
      "Epoch 65, Batch 2500, Loss: 0.2032\n",
      "Epoch 65, Batch 2600, Loss: 0.1787\n",
      "Epoch 65, Batch 2700, Loss: 0.1899\n",
      "Epoch 65, Batch 2800, Loss: 0.1708\n",
      "Epoch 65, Batch 2900, Loss: 0.1907\n",
      "Epoch 66, Batch 100, Loss: 0.1717\n",
      "Epoch 66, Batch 200, Loss: 0.1802\n",
      "Epoch 66, Batch 300, Loss: 0.1849\n",
      "Epoch 66, Batch 400, Loss: 0.1920\n",
      "Epoch 66, Batch 500, Loss: 0.1813\n",
      "Epoch 66, Batch 600, Loss: 0.1744\n",
      "Epoch 66, Batch 700, Loss: 0.1827\n",
      "Epoch 66, Batch 800, Loss: 0.1698\n",
      "Epoch 66, Batch 900, Loss: 0.1729\n",
      "Epoch 66, Batch 1000, Loss: 0.1831\n",
      "Epoch 66, Batch 1100, Loss: 0.1958\n",
      "Epoch 66, Batch 1200, Loss: 0.1820\n",
      "Epoch 66, Batch 1300, Loss: 0.1783\n",
      "Epoch 66, Batch 1400, Loss: 0.1859\n",
      "Epoch 66, Batch 1500, Loss: 0.1689\n",
      "Epoch 66, Batch 1600, Loss: 0.1743\n",
      "Epoch 66, Batch 1700, Loss: 0.1723\n",
      "Epoch 66, Batch 1800, Loss: 0.1809\n",
      "Epoch 66, Batch 1900, Loss: 0.1641\n",
      "Epoch 66, Batch 2000, Loss: 0.1814\n",
      "Epoch 66, Batch 2100, Loss: 0.1782\n",
      "Epoch 66, Batch 2200, Loss: 0.1911\n",
      "Epoch 66, Batch 2300, Loss: 0.1793\n",
      "Epoch 66, Batch 2400, Loss: 0.1827\n",
      "Epoch 66, Batch 2500, Loss: 0.1796\n",
      "Epoch 66, Batch 2600, Loss: 0.1837\n",
      "Epoch 66, Batch 2700, Loss: 0.1901\n",
      "Epoch 66, Batch 2800, Loss: 0.1874\n",
      "Epoch 66, Batch 2900, Loss: 0.1947\n",
      "Epoch 67, Batch 100, Loss: 0.1677\n",
      "Epoch 67, Batch 200, Loss: 0.1746\n",
      "Epoch 67, Batch 300, Loss: 0.1885\n",
      "Epoch 67, Batch 400, Loss: 0.1678\n",
      "Epoch 67, Batch 500, Loss: 0.1636\n",
      "Epoch 67, Batch 600, Loss: 0.1785\n",
      "Epoch 67, Batch 700, Loss: 0.1637\n",
      "Epoch 67, Batch 800, Loss: 0.1882\n",
      "Epoch 67, Batch 900, Loss: 0.1784\n",
      "Epoch 67, Batch 1000, Loss: 0.1857\n",
      "Epoch 67, Batch 1100, Loss: 0.1888\n",
      "Epoch 67, Batch 1200, Loss: 0.1822\n",
      "Epoch 67, Batch 1300, Loss: 0.1885\n",
      "Epoch 67, Batch 1400, Loss: 0.1910\n",
      "Epoch 67, Batch 1500, Loss: 0.1784\n",
      "Epoch 67, Batch 1600, Loss: 0.1890\n",
      "Epoch 67, Batch 1700, Loss: 0.1819\n",
      "Epoch 67, Batch 1800, Loss: 0.1799\n",
      "Epoch 67, Batch 1900, Loss: 0.1781\n",
      "Epoch 67, Batch 2000, Loss: 0.1845\n",
      "Epoch 67, Batch 2100, Loss: 0.1780\n",
      "Epoch 67, Batch 2200, Loss: 0.1876\n",
      "Epoch 67, Batch 2300, Loss: 0.1808\n",
      "Epoch 67, Batch 2400, Loss: 0.1797\n",
      "Epoch 67, Batch 2500, Loss: 0.1909\n",
      "Epoch 67, Batch 2600, Loss: 0.1746\n",
      "Epoch 67, Batch 2700, Loss: 0.1880\n",
      "Epoch 67, Batch 2800, Loss: 0.1668\n",
      "Epoch 67, Batch 2900, Loss: 0.1843\n",
      "Epoch 68, Batch 100, Loss: 0.1588\n",
      "Epoch 68, Batch 200, Loss: 0.1757\n",
      "Epoch 68, Batch 300, Loss: 0.1734\n",
      "Epoch 68, Batch 400, Loss: 0.1780\n",
      "Epoch 68, Batch 500, Loss: 0.1716\n",
      "Epoch 68, Batch 600, Loss: 0.1815\n",
      "Epoch 68, Batch 700, Loss: 0.1820\n",
      "Epoch 68, Batch 800, Loss: 0.1644\n",
      "Epoch 68, Batch 900, Loss: 0.1842\n",
      "Epoch 68, Batch 1000, Loss: 0.1645\n",
      "Epoch 68, Batch 1100, Loss: 0.1747\n",
      "Epoch 68, Batch 1200, Loss: 0.1733\n",
      "Epoch 68, Batch 1300, Loss: 0.1771\n",
      "Epoch 68, Batch 1400, Loss: 0.1768\n",
      "Epoch 68, Batch 1500, Loss: 0.1835\n",
      "Epoch 68, Batch 1600, Loss: 0.1741\n",
      "Epoch 68, Batch 1700, Loss: 0.1662\n",
      "Epoch 68, Batch 1800, Loss: 0.1937\n",
      "Epoch 68, Batch 1900, Loss: 0.1822\n",
      "Epoch 68, Batch 2000, Loss: 0.1841\n",
      "Epoch 68, Batch 2100, Loss: 0.1879\n",
      "Epoch 68, Batch 2200, Loss: 0.1833\n",
      "Epoch 68, Batch 2300, Loss: 0.1827\n",
      "Epoch 68, Batch 2400, Loss: 0.1787\n",
      "Epoch 68, Batch 2500, Loss: 0.1754\n",
      "Epoch 68, Batch 2600, Loss: 0.1835\n",
      "Epoch 68, Batch 2700, Loss: 0.1869\n",
      "Epoch 68, Batch 2800, Loss: 0.1965\n",
      "Epoch 68, Batch 2900, Loss: 0.1774\n",
      "Epoch 69, Batch 100, Loss: 0.1574\n",
      "Epoch 69, Batch 200, Loss: 0.1709\n",
      "Epoch 69, Batch 300, Loss: 0.1787\n",
      "Epoch 69, Batch 400, Loss: 0.1766\n",
      "Epoch 69, Batch 500, Loss: 0.1654\n",
      "Epoch 69, Batch 600, Loss: 0.1740\n",
      "Epoch 69, Batch 700, Loss: 0.1724\n",
      "Epoch 69, Batch 800, Loss: 0.1706\n",
      "Epoch 69, Batch 900, Loss: 0.1707\n",
      "Epoch 69, Batch 1000, Loss: 0.1830\n",
      "Epoch 69, Batch 1100, Loss: 0.1865\n",
      "Epoch 69, Batch 1200, Loss: 0.1817\n",
      "Epoch 69, Batch 1300, Loss: 0.1797\n",
      "Epoch 69, Batch 1400, Loss: 0.1748\n",
      "Epoch 69, Batch 1500, Loss: 0.1818\n",
      "Epoch 69, Batch 1600, Loss: 0.1556\n",
      "Epoch 69, Batch 1700, Loss: 0.1607\n",
      "Epoch 69, Batch 1800, Loss: 0.1760\n",
      "Epoch 69, Batch 1900, Loss: 0.1830\n",
      "Epoch 69, Batch 2000, Loss: 0.1705\n",
      "Epoch 69, Batch 2100, Loss: 0.1777\n",
      "Epoch 69, Batch 2200, Loss: 0.1885\n",
      "Epoch 69, Batch 2300, Loss: 0.1768\n",
      "Epoch 69, Batch 2400, Loss: 0.2042\n",
      "Epoch 69, Batch 2500, Loss: 0.1682\n",
      "Epoch 69, Batch 2600, Loss: 0.1876\n",
      "Epoch 69, Batch 2700, Loss: 0.1646\n",
      "Epoch 69, Batch 2800, Loss: 0.1800\n",
      "Epoch 69, Batch 2900, Loss: 0.1772\n",
      "Epoch 70, Batch 100, Loss: 0.1756\n",
      "Epoch 70, Batch 200, Loss: 0.1664\n",
      "Epoch 70, Batch 300, Loss: 0.1546\n",
      "Epoch 70, Batch 400, Loss: 0.1709\n",
      "Epoch 70, Batch 500, Loss: 0.1694\n",
      "Epoch 70, Batch 600, Loss: 0.1840\n",
      "Epoch 70, Batch 700, Loss: 0.1669\n",
      "Epoch 70, Batch 800, Loss: 0.1775\n",
      "Epoch 70, Batch 900, Loss: 0.1654\n",
      "Epoch 70, Batch 1000, Loss: 0.1611\n",
      "Epoch 70, Batch 1100, Loss: 0.1819\n",
      "Epoch 70, Batch 1200, Loss: 0.1663\n",
      "Epoch 70, Batch 1300, Loss: 0.1716\n",
      "Epoch 70, Batch 1400, Loss: 0.1815\n",
      "Epoch 70, Batch 1500, Loss: 0.1862\n",
      "Epoch 70, Batch 1600, Loss: 0.1774\n",
      "Epoch 70, Batch 1700, Loss: 0.1806\n",
      "Epoch 70, Batch 1800, Loss: 0.1766\n",
      "Epoch 70, Batch 1900, Loss: 0.1629\n",
      "Epoch 70, Batch 2000, Loss: 0.1791\n",
      "Epoch 70, Batch 2100, Loss: 0.1732\n",
      "Epoch 70, Batch 2200, Loss: 0.1876\n",
      "Epoch 70, Batch 2300, Loss: 0.1556\n",
      "Epoch 70, Batch 2400, Loss: 0.1662\n",
      "Epoch 70, Batch 2500, Loss: 0.1836\n",
      "Epoch 70, Batch 2600, Loss: 0.1768\n",
      "Epoch 70, Batch 2700, Loss: 0.1644\n",
      "Epoch 70, Batch 2800, Loss: 0.1825\n",
      "Epoch 70, Batch 2900, Loss: 0.1643\n",
      "Epoch 71, Batch 100, Loss: 0.1668\n",
      "Epoch 71, Batch 200, Loss: 0.1890\n",
      "Epoch 71, Batch 300, Loss: 0.1647\n",
      "Epoch 71, Batch 400, Loss: 0.1701\n",
      "Epoch 71, Batch 500, Loss: 0.1836\n",
      "Epoch 71, Batch 600, Loss: 0.1650\n",
      "Epoch 71, Batch 700, Loss: 0.1750\n",
      "Epoch 71, Batch 800, Loss: 0.1847\n",
      "Epoch 71, Batch 900, Loss: 0.1738\n",
      "Epoch 71, Batch 1000, Loss: 0.1783\n",
      "Epoch 71, Batch 1100, Loss: 0.1646\n",
      "Epoch 71, Batch 1200, Loss: 0.1647\n",
      "Epoch 71, Batch 1300, Loss: 0.1589\n",
      "Epoch 71, Batch 1400, Loss: 0.1649\n",
      "Epoch 71, Batch 1500, Loss: 0.1655\n",
      "Epoch 71, Batch 1600, Loss: 0.1784\n",
      "Epoch 71, Batch 1700, Loss: 0.1723\n",
      "Epoch 71, Batch 1800, Loss: 0.1670\n",
      "Epoch 71, Batch 1900, Loss: 0.1684\n",
      "Epoch 71, Batch 2000, Loss: 0.1863\n",
      "Epoch 71, Batch 2100, Loss: 0.1685\n",
      "Epoch 71, Batch 2200, Loss: 0.1672\n",
      "Epoch 71, Batch 2300, Loss: 0.1726\n",
      "Epoch 71, Batch 2400, Loss: 0.1791\n",
      "Epoch 71, Batch 2500, Loss: 0.1700\n",
      "Epoch 71, Batch 2600, Loss: 0.1653\n",
      "Epoch 71, Batch 2700, Loss: 0.1672\n",
      "Epoch 71, Batch 2800, Loss: 0.1877\n",
      "Epoch 71, Batch 2900, Loss: 0.1702\n",
      "Epoch 72, Batch 100, Loss: 0.1541\n",
      "Epoch 72, Batch 200, Loss: 0.1575\n",
      "Epoch 72, Batch 300, Loss: 0.1478\n",
      "Epoch 72, Batch 400, Loss: 0.1811\n",
      "Epoch 72, Batch 500, Loss: 0.1582\n",
      "Epoch 72, Batch 600, Loss: 0.1767\n",
      "Epoch 72, Batch 700, Loss: 0.1637\n",
      "Epoch 72, Batch 800, Loss: 0.1615\n",
      "Epoch 72, Batch 900, Loss: 0.1922\n",
      "Epoch 72, Batch 1000, Loss: 0.1828\n",
      "Epoch 72, Batch 1100, Loss: 0.1701\n",
      "Epoch 72, Batch 1200, Loss: 0.1681\n",
      "Epoch 72, Batch 1300, Loss: 0.1645\n",
      "Epoch 72, Batch 1400, Loss: 0.1627\n",
      "Epoch 72, Batch 1500, Loss: 0.1549\n",
      "Epoch 72, Batch 1600, Loss: 0.1604\n",
      "Epoch 72, Batch 1700, Loss: 0.1716\n",
      "Epoch 72, Batch 1800, Loss: 0.1803\n",
      "Epoch 72, Batch 1900, Loss: 0.1519\n",
      "Epoch 72, Batch 2000, Loss: 0.1578\n",
      "Epoch 72, Batch 2100, Loss: 0.1817\n",
      "Epoch 72, Batch 2200, Loss: 0.1623\n",
      "Epoch 72, Batch 2300, Loss: 0.1710\n",
      "Epoch 72, Batch 2400, Loss: 0.1929\n",
      "Epoch 72, Batch 2500, Loss: 0.1690\n",
      "Epoch 72, Batch 2600, Loss: 0.1758\n",
      "Epoch 72, Batch 2700, Loss: 0.1767\n",
      "Epoch 72, Batch 2800, Loss: 0.1829\n",
      "Epoch 72, Batch 2900, Loss: 0.1802\n",
      "Epoch 73, Batch 100, Loss: 0.1852\n",
      "Epoch 73, Batch 200, Loss: 0.1506\n",
      "Epoch 73, Batch 300, Loss: 0.1569\n",
      "Epoch 73, Batch 400, Loss: 0.1685\n",
      "Epoch 73, Batch 500, Loss: 0.1546\n",
      "Epoch 73, Batch 600, Loss: 0.1715\n",
      "Epoch 73, Batch 700, Loss: 0.1637\n",
      "Epoch 73, Batch 800, Loss: 0.1742\n",
      "Epoch 73, Batch 900, Loss: 0.1704\n",
      "Epoch 73, Batch 1000, Loss: 0.1588\n",
      "Epoch 73, Batch 1100, Loss: 0.1723\n",
      "Epoch 73, Batch 1200, Loss: 0.1720\n",
      "Epoch 73, Batch 1300, Loss: 0.1685\n",
      "Epoch 73, Batch 1400, Loss: 0.1729\n",
      "Epoch 73, Batch 1500, Loss: 0.1596\n",
      "Epoch 73, Batch 1600, Loss: 0.1622\n",
      "Epoch 73, Batch 1700, Loss: 0.1772\n",
      "Epoch 73, Batch 1800, Loss: 0.1622\n",
      "Epoch 73, Batch 1900, Loss: 0.1732\n",
      "Epoch 73, Batch 2000, Loss: 0.1546\n",
      "Epoch 73, Batch 2100, Loss: 0.1662\n",
      "Epoch 73, Batch 2200, Loss: 0.1704\n",
      "Epoch 73, Batch 2300, Loss: 0.1761\n",
      "Epoch 73, Batch 2400, Loss: 0.1677\n",
      "Epoch 73, Batch 2500, Loss: 0.1739\n",
      "Epoch 73, Batch 2600, Loss: 0.1700\n",
      "Epoch 73, Batch 2700, Loss: 0.1707\n",
      "Epoch 73, Batch 2800, Loss: 0.1937\n",
      "Epoch 73, Batch 2900, Loss: 0.1678\n",
      "Epoch 74, Batch 100, Loss: 0.1646\n",
      "Epoch 74, Batch 200, Loss: 0.1665\n",
      "Epoch 74, Batch 300, Loss: 0.1693\n",
      "Epoch 74, Batch 400, Loss: 0.1668\n",
      "Epoch 74, Batch 500, Loss: 0.1621\n",
      "Epoch 74, Batch 600, Loss: 0.1676\n",
      "Epoch 74, Batch 700, Loss: 0.1505\n",
      "Epoch 74, Batch 800, Loss: 0.1647\n",
      "Epoch 74, Batch 900, Loss: 0.1599\n",
      "Epoch 74, Batch 1000, Loss: 0.1480\n",
      "Epoch 74, Batch 1100, Loss: 0.1812\n",
      "Epoch 74, Batch 1200, Loss: 0.1637\n",
      "Epoch 74, Batch 1300, Loss: 0.1726\n",
      "Epoch 74, Batch 1400, Loss: 0.1554\n",
      "Epoch 74, Batch 1500, Loss: 0.1607\n",
      "Epoch 74, Batch 1600, Loss: 0.1827\n",
      "Epoch 74, Batch 1700, Loss: 0.1632\n",
      "Epoch 74, Batch 1800, Loss: 0.1763\n",
      "Epoch 74, Batch 1900, Loss: 0.1554\n",
      "Epoch 74, Batch 2000, Loss: 0.1700\n",
      "Epoch 74, Batch 2100, Loss: 0.1649\n",
      "Epoch 74, Batch 2200, Loss: 0.1763\n",
      "Epoch 74, Batch 2300, Loss: 0.1723\n",
      "Epoch 74, Batch 2400, Loss: 0.1831\n",
      "Epoch 74, Batch 2500, Loss: 0.1739\n",
      "Epoch 74, Batch 2600, Loss: 0.1785\n",
      "Epoch 74, Batch 2700, Loss: 0.1761\n",
      "Epoch 74, Batch 2800, Loss: 0.1803\n",
      "Epoch 74, Batch 2900, Loss: 0.1809\n",
      "Epoch 75, Batch 100, Loss: 0.1465\n",
      "Epoch 75, Batch 200, Loss: 0.1588\n",
      "Epoch 75, Batch 300, Loss: 0.1681\n",
      "Epoch 75, Batch 400, Loss: 0.1649\n",
      "Epoch 75, Batch 500, Loss: 0.1697\n",
      "Epoch 75, Batch 600, Loss: 0.1524\n",
      "Epoch 75, Batch 700, Loss: 0.1658\n",
      "Epoch 75, Batch 800, Loss: 0.1609\n",
      "Epoch 75, Batch 900, Loss: 0.1705\n",
      "Epoch 75, Batch 1000, Loss: 0.1661\n",
      "Epoch 75, Batch 1100, Loss: 0.1697\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:76\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:53\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, criterion, optimizer, epochs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/rag/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rag/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/rag/lib/python3.11/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/rag/lib/python3.11/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rag/lib/python3.11/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define the dataset class\n",
    "class RatingDataset(Dataset):\n",
    "    \"\"\"Dataset for loading user-item ratings for training\"\"\"\n",
    "    def __init__(self, user_ids, item_ids, ratings):\n",
    "        self.user_ids = torch.tensor(user_ids, dtype=torch.int64)\n",
    "        self.item_ids = torch.tensor(item_ids, dtype=torch.int64)\n",
    "        self.ratings = torch.tensor(ratings, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.item_ids[idx], self.ratings[idx]\n",
    "\n",
    "# Define the NCF model\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, factors=20, hidden_layers=[64, 32, 16], dropout=0.2):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, factors)\n",
    "        self.item_embedding = nn.Embedding(num_items, factors)\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        input_size = factors * 2  # Concatenate user and item embeddings\n",
    "        for hidden_layer in hidden_layers:\n",
    "            self.fc_layers.append(nn.Linear(input_size, hidden_layer))\n",
    "            input_size = hidden_layer\n",
    "        self.output = nn.Linear(input_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding = self.user_embedding(user_indices)\n",
    "        item_embedding = self.item_embedding(item_indices)\n",
    "        x = torch.cat([user_embedding, item_embedding], dim=-1)\n",
    "        for layer in self.fc_layers:\n",
    "            x = self.relu(layer(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "# Training function\n",
    "def train_model(model, data_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (users, items, ratings) in enumerate(data_loader):\n",
    "            users = users.to(device)  # Move data to GPU\n",
    "            items = items.to(device)  # Move data to GPU\n",
    "            ratings = ratings.to(device)  # Move data to GPU\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(users, items)\n",
    "            loss = criterion(outputs, ratings)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if batch_idx % 100 == 99:\n",
    "                print(f'Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {running_loss / 100:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "# Prepare data\n",
    "train_data = pd.read_csv(\"cs608_ip_train_v3.csv\")\n",
    "train_data['user_id'] = train_data['user_id'].astype('category').cat.codes\n",
    "train_data['item_id'] = train_data['item_id'].astype('category').cat.codes\n",
    "dataset = RatingDataset(train_data['user_id'], train_data['item_id'], train_data['rating'])\n",
    "\n",
    "# Create DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "num_users = train_data['user_id'].nunique()\n",
    "num_items = train_data['item_id'].nunique()\n",
    "model = NCF(num_users, num_items).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, data_loader, criterion, optimizer, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# Load your CSV data\n",
    "data_path = \"./cs608_ip_probe_v3.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Assume df has columns 'user_id', 'item_id', which we need to convert to tensor\n",
    "# Also assume that 'ratings' column is your target\n",
    "users = torch.tensor(df[\"user_id\"].values).to(device)\n",
    "items = torch.tensor(df[\"item_id\"].values).to(device)\n",
    "ratings = torch.tensor(df[\"rating\"].values)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Create a data loader for batch processing\n",
    "dataset = TensorDataset(users, items, ratings)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# To store predictions and actual values\n",
    "predictions, actuals = [], []\n",
    "\n",
    "# Evaluate the model\n",
    "for user, item, rating in data_loader:\n",
    "    with torch.no_grad():\n",
    "        output = model(user, item)\n",
    "        predictions.extend(output.cpu().numpy())\n",
    "        actuals.extend(rating.cpu().numpy())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "predictions = np.array(predictions)\n",
    "actuals = np.array(actuals)\n",
    "rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "accuracy = accuracy_score(actuals, predictions.round())\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def generate_recommendations(model, num_users, num_items, top_k=50):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    recommendations = []\n",
    "\n",
    "    # Iterate over all users\n",
    "    for user_id in tqdm(range(num_users)):\n",
    "        user_tensor = torch.tensor(\n",
    "            [user_id] * num_items, dtype=torch.int64\n",
    "        ).to(device)  # Repeat user ID for each item\n",
    "        item_tensor = torch.tensor(range(num_items), dtype=torch.int64).to(device)  # All item IDs\n",
    "\n",
    "        # Predict scores for all items for this user\n",
    "        with torch.no_grad():\n",
    "            scores = (\n",
    "                model(user_tensor, item_tensor).cpu().numpy()\n",
    "            )  # Get scores and move to CPU\n",
    "\n",
    "        # Get the indices of the top k scores\n",
    "        top_item_indices = scores.argsort()[-top_k:][\n",
    "            ::-1\n",
    "        ]  # Indices of top scoring items\n",
    "\n",
    "        # Append to the list of recommendations\n",
    "        recommendations.append(top_item_indices.tolist())\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1024f2444ad14d66bafe9aa185508338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 16s\n",
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# Number of users and items\n",
    "num_users = train_data[\"user_id\"].nunique()\n",
    "num_items = train_data[\"item_id\"].nunique()\n",
    "\n",
    "# Generate recommendations for all users\n",
    "top_k_recommendations = generate_recommendations(model, num_users, num_items)\n",
    "\n",
    "with open(\"submission.txt\", \"w\") as file:\n",
    "    for user_recommendations in top_k_recommendations:\n",
    "        file.write(\" \".join(map(str, user_recommendations)) + \"\\n\")\n",
    "\n",
    "# zip the submission file\n",
    "with zipfile.ZipFile('submission.zip', 'w') as file:\n",
    "    file.write('submission.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
