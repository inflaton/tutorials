{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS not available because the current PyTorch install was not built with MPS enabled.\n",
      "Using GPU: NVIDIA GeForce RTX 4080 Laptop GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader  \n",
    "\n",
    "# Check that MPS is available\n",
    "\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\n",
    "            \"MPS not available because the current PyTorch install was not \"\n",
    "            \"built with MPS enabled.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "            \"and/or you do not have an MPS-enabled device on this machine.\"\n",
    "        )\n",
    "    mps_device = None\n",
    "else:\n",
    "    mps_device = torch.device(\"mps\")\n",
    "\n",
    "if mps_device is not None:\n",
    "    device = mps_device\n",
    "    print(\"Using MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 8.7643\n",
      "Epoch 1, Batch 200, Loss: 2.6973\n",
      "Epoch 1, Batch 300, Loss: 2.5343\n",
      "Epoch 1, Batch 400, Loss: 2.5485\n",
      "Epoch 1, Batch 500, Loss: 2.4644\n",
      "Epoch 1, Batch 600, Loss: 2.4243\n",
      "Epoch 1, Batch 700, Loss: 2.3473\n",
      "Epoch 1, Batch 800, Loss: 2.2627\n",
      "Epoch 1, Batch 900, Loss: 2.2678\n",
      "Epoch 1, Batch 1000, Loss: 2.1257\n",
      "Epoch 1, Batch 1100, Loss: 2.1159\n",
      "Epoch 1, Batch 1200, Loss: 2.0700\n",
      "Epoch 1, Batch 1300, Loss: 2.0703\n",
      "Epoch 1, Batch 1400, Loss: 2.0524\n",
      "Epoch 1, Batch 1500, Loss: 1.9497\n",
      "Epoch 1, Batch 1600, Loss: 1.9917\n",
      "Epoch 1, Batch 1700, Loss: 2.0041\n",
      "Epoch 1, Batch 1800, Loss: 1.9235\n",
      "Epoch 1, Batch 1900, Loss: 1.9449\n",
      "Epoch 1, Batch 2000, Loss: 1.8351\n",
      "Epoch 1, Batch 2100, Loss: 1.8814\n",
      "Epoch 1, Batch 2200, Loss: 1.7991\n",
      "Epoch 1, Batch 2300, Loss: 1.8287\n",
      "Epoch 1, Batch 2400, Loss: 1.7909\n",
      "Epoch 1, Batch 2500, Loss: 1.8362\n",
      "Epoch 1, Batch 2600, Loss: 1.7520\n",
      "Epoch 1, Batch 2700, Loss: 1.7595\n",
      "Epoch 1, Batch 2800, Loss: 1.7768\n",
      "Epoch 1, Batch 2900, Loss: 1.6970\n",
      "Epoch 2, Batch 100, Loss: 1.6922\n",
      "Epoch 2, Batch 200, Loss: 1.7097\n",
      "Epoch 2, Batch 300, Loss: 1.7444\n",
      "Epoch 2, Batch 400, Loss: 1.6588\n",
      "Epoch 2, Batch 500, Loss: 1.6890\n",
      "Epoch 2, Batch 600, Loss: 1.6703\n",
      "Epoch 2, Batch 700, Loss: 1.6324\n",
      "Epoch 2, Batch 800, Loss: 1.6495\n",
      "Epoch 2, Batch 900, Loss: 1.6191\n",
      "Epoch 2, Batch 1000, Loss: 1.6198\n",
      "Epoch 2, Batch 1100, Loss: 1.6593\n",
      "Epoch 2, Batch 1200, Loss: 1.6102\n",
      "Epoch 2, Batch 1300, Loss: 1.6433\n",
      "Epoch 2, Batch 1400, Loss: 1.5941\n",
      "Epoch 2, Batch 1500, Loss: 1.6741\n",
      "Epoch 2, Batch 1600, Loss: 1.6032\n",
      "Epoch 2, Batch 1700, Loss: 1.5716\n",
      "Epoch 2, Batch 1800, Loss: 1.5541\n",
      "Epoch 2, Batch 1900, Loss: 1.6738\n",
      "Epoch 2, Batch 2000, Loss: 1.5800\n",
      "Epoch 2, Batch 2100, Loss: 1.5202\n",
      "Epoch 2, Batch 2200, Loss: 1.5658\n",
      "Epoch 2, Batch 2300, Loss: 1.4885\n",
      "Epoch 2, Batch 2400, Loss: 1.5808\n",
      "Epoch 2, Batch 2500, Loss: 1.5535\n",
      "Epoch 2, Batch 2600, Loss: 1.5263\n",
      "Epoch 2, Batch 2700, Loss: 1.5257\n",
      "Epoch 2, Batch 2800, Loss: 1.4675\n",
      "Epoch 2, Batch 2900, Loss: 1.5338\n",
      "Epoch 3, Batch 100, Loss: 1.4608\n",
      "Epoch 3, Batch 200, Loss: 1.5429\n",
      "Epoch 3, Batch 300, Loss: 1.5135\n",
      "Epoch 3, Batch 400, Loss: 1.4907\n",
      "Epoch 3, Batch 500, Loss: 1.4910\n",
      "Epoch 3, Batch 600, Loss: 1.4915\n",
      "Epoch 3, Batch 700, Loss: 1.5017\n",
      "Epoch 3, Batch 800, Loss: 1.5212\n",
      "Epoch 3, Batch 900, Loss: 1.4750\n",
      "Epoch 3, Batch 1000, Loss: 1.4829\n",
      "Epoch 3, Batch 1100, Loss: 1.4282\n",
      "Epoch 3, Batch 1200, Loss: 1.5027\n",
      "Epoch 3, Batch 1300, Loss: 1.4650\n",
      "Epoch 3, Batch 1400, Loss: 1.4684\n",
      "Epoch 3, Batch 1500, Loss: 1.4359\n",
      "Epoch 3, Batch 1600, Loss: 1.4803\n",
      "Epoch 3, Batch 1700, Loss: 1.3891\n",
      "Epoch 3, Batch 1800, Loss: 1.4129\n",
      "Epoch 3, Batch 1900, Loss: 1.3981\n",
      "Epoch 3, Batch 2000, Loss: 1.4631\n",
      "Epoch 3, Batch 2100, Loss: 1.4122\n",
      "Epoch 3, Batch 2200, Loss: 1.4588\n",
      "Epoch 3, Batch 2300, Loss: 1.4447\n",
      "Epoch 3, Batch 2400, Loss: 1.4211\n",
      "Epoch 3, Batch 2500, Loss: 1.4311\n",
      "Epoch 3, Batch 2600, Loss: 1.4255\n",
      "Epoch 3, Batch 2700, Loss: 1.4177\n",
      "Epoch 3, Batch 2800, Loss: 1.4611\n",
      "Epoch 3, Batch 2900, Loss: 1.4346\n",
      "Epoch 4, Batch 100, Loss: 1.3965\n",
      "Epoch 4, Batch 200, Loss: 1.3636\n",
      "Epoch 4, Batch 300, Loss: 1.3588\n",
      "Epoch 4, Batch 400, Loss: 1.3267\n",
      "Epoch 4, Batch 500, Loss: 1.3346\n",
      "Epoch 4, Batch 600, Loss: 1.3852\n",
      "Epoch 4, Batch 700, Loss: 1.3919\n",
      "Epoch 4, Batch 800, Loss: 1.4292\n",
      "Epoch 4, Batch 900, Loss: 1.2975\n",
      "Epoch 4, Batch 1000, Loss: 1.4468\n",
      "Epoch 4, Batch 1100, Loss: 1.3737\n",
      "Epoch 4, Batch 1200, Loss: 1.3566\n",
      "Epoch 4, Batch 1300, Loss: 1.3232\n",
      "Epoch 4, Batch 1400, Loss: 1.3455\n",
      "Epoch 4, Batch 1500, Loss: 1.3153\n",
      "Epoch 4, Batch 1600, Loss: 1.3530\n",
      "Epoch 4, Batch 1700, Loss: 1.3431\n",
      "Epoch 4, Batch 1800, Loss: 1.3079\n",
      "Epoch 4, Batch 1900, Loss: 1.3128\n",
      "Epoch 4, Batch 2000, Loss: 1.3404\n",
      "Epoch 4, Batch 2100, Loss: 1.3467\n",
      "Epoch 4, Batch 2200, Loss: 1.2952\n",
      "Epoch 4, Batch 2300, Loss: 1.3611\n",
      "Epoch 4, Batch 2400, Loss: 1.3137\n",
      "Epoch 4, Batch 2500, Loss: 1.2925\n",
      "Epoch 4, Batch 2600, Loss: 1.3428\n",
      "Epoch 4, Batch 2700, Loss: 1.3446\n",
      "Epoch 4, Batch 2800, Loss: 1.3166\n",
      "Epoch 4, Batch 2900, Loss: 1.3172\n",
      "Epoch 5, Batch 100, Loss: 1.2764\n",
      "Epoch 5, Batch 200, Loss: 1.2524\n",
      "Epoch 5, Batch 300, Loss: 1.2453\n",
      "Epoch 5, Batch 400, Loss: 1.2780\n",
      "Epoch 5, Batch 500, Loss: 1.2619\n",
      "Epoch 5, Batch 600, Loss: 1.2609\n",
      "Epoch 5, Batch 700, Loss: 1.2396\n",
      "Epoch 5, Batch 800, Loss: 1.2237\n",
      "Epoch 5, Batch 900, Loss: 1.2491\n",
      "Epoch 5, Batch 1000, Loss: 1.1952\n",
      "Epoch 5, Batch 1100, Loss: 1.2714\n",
      "Epoch 5, Batch 1200, Loss: 1.2052\n",
      "Epoch 5, Batch 1300, Loss: 1.2792\n",
      "Epoch 5, Batch 1400, Loss: 1.2137\n",
      "Epoch 5, Batch 1500, Loss: 1.1801\n",
      "Epoch 5, Batch 1600, Loss: 1.2207\n",
      "Epoch 5, Batch 1700, Loss: 1.2090\n",
      "Epoch 5, Batch 1800, Loss: 1.1947\n",
      "Epoch 5, Batch 1900, Loss: 1.2074\n",
      "Epoch 5, Batch 2000, Loss: 1.2609\n",
      "Epoch 5, Batch 2100, Loss: 1.2450\n",
      "Epoch 5, Batch 2200, Loss: 1.2812\n",
      "Epoch 5, Batch 2300, Loss: 1.2200\n",
      "Epoch 5, Batch 2400, Loss: 1.2257\n",
      "Epoch 5, Batch 2500, Loss: 1.2457\n",
      "Epoch 5, Batch 2600, Loss: 1.2801\n",
      "Epoch 5, Batch 2700, Loss: 1.2295\n",
      "Epoch 5, Batch 2800, Loss: 1.2049\n",
      "Epoch 5, Batch 2900, Loss: 1.1955\n",
      "CPU times: total: 11.5 s\n",
      "Wall time: 40.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define the dataset class\n",
    "class RatingDataset(Dataset):\n",
    "    \"\"\"Dataset for loading user-item ratings for training\"\"\"\n",
    "    def __init__(self, user_ids, item_ids, ratings):\n",
    "        self.user_ids = torch.tensor(user_ids, dtype=torch.int64)\n",
    "        self.item_ids = torch.tensor(item_ids, dtype=torch.int64)\n",
    "        self.ratings = torch.tensor(ratings, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.item_ids[idx], self.ratings[idx]\n",
    "\n",
    "# Define the NCF model\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, factors=20, hidden_layers=[64, 32, 16], dropout=0.2):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, factors)\n",
    "        self.item_embedding = nn.Embedding(num_items, factors)\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        input_size = factors * 2  # Concatenate user and item embeddings\n",
    "        for hidden_layer in hidden_layers:\n",
    "            self.fc_layers.append(nn.Linear(input_size, hidden_layer))\n",
    "            input_size = hidden_layer\n",
    "        self.output = nn.Linear(input_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding = self.user_embedding(user_indices)\n",
    "        item_embedding = self.item_embedding(item_indices)\n",
    "        x = torch.cat([user_embedding, item_embedding], dim=-1)\n",
    "        for layer in self.fc_layers:\n",
    "            x = self.relu(layer(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "# Training function\n",
    "def train_model(model, data_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (users, items, ratings) in enumerate(data_loader):\n",
    "            users = users.to(device)  # Move data to GPU\n",
    "            items = items.to(device)  # Move data to GPU\n",
    "            ratings = ratings.to(device)  # Move data to GPU\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(users, items)\n",
    "            loss = criterion(outputs, ratings)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if batch_idx % 100 == 99:\n",
    "                print(f'Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {running_loss / 100:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "# Prepare data\n",
    "train_data = pd.read_csv(\"cs608_ip_train_v3.csv\")\n",
    "train_data['user_id'] = train_data['user_id'].astype('category').cat.codes\n",
    "train_data['item_id'] = train_data['item_id'].astype('category').cat.codes\n",
    "dataset = RatingDataset(train_data['user_id'], train_data['item_id'], train_data['rating'])\n",
    "\n",
    "# Create DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "num_users = train_data['user_id'].nunique()\n",
    "num_items = train_data['item_id'].nunique()\n",
    "model = NCF(num_users, num_items).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, data_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def generate_recommendations(model, num_users, num_items, top_k=50):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    recommendations = []\n",
    "\n",
    "    # Iterate over all users\n",
    "    for user_id in tqdm(range(num_users)):\n",
    "        user_tensor = torch.tensor(\n",
    "            [user_id] * num_items, dtype=torch.int64\n",
    "        ).to(device)  # Repeat user ID for each item\n",
    "        item_tensor = torch.tensor(range(num_items), dtype=torch.int64).to(device)  # All item IDs\n",
    "\n",
    "        # Predict scores for all items for this user\n",
    "        with torch.no_grad():\n",
    "            scores = (\n",
    "                model(user_tensor, item_tensor).cpu().numpy()\n",
    "            )  # Get scores and move to CPU\n",
    "\n",
    "        # Get the indices of the top k scores\n",
    "        top_item_indices = scores.argsort()[-top_k:][\n",
    "            ::-1\n",
    "        ]  # Indices of top scoring items\n",
    "\n",
    "        # Append to the list of recommendations\n",
    "        recommendations.append(top_item_indices.tolist())\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d80405613c45b79e119d09c8e2cc47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min\n",
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# Number of users and items\n",
    "num_users = train_data[\"user_id\"].nunique()\n",
    "num_items = train_data[\"item_id\"].nunique()\n",
    "\n",
    "# Generate recommendations for all users\n",
    "top_k_recommendations = generate_recommendations(model, num_users, num_items)\n",
    "\n",
    "with open(\"submission.txt\", \"w\") as file:\n",
    "    for user_recommendations in top_k_recommendations:\n",
    "        file.write(\" \".join(map(str, user_recommendations)) + \"\\n\")\n",
    "\n",
    "# zip the submission file\n",
    "with zipfile.ZipFile('submission.zip', 'w') as file:\n",
    "    file.write('submission.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
