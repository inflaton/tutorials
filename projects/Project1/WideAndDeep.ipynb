{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader  \n",
    "\n",
    "# Check that MPS is available\n",
    "\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\n",
    "            \"MPS not available because the current PyTorch install was not \"\n",
    "            \"built with MPS enabled.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "            \"and/or you do not have an MPS-enabled device on this machine.\"\n",
    "        )\n",
    "    mps_device = None\n",
    "else:\n",
    "    mps_device = torch.device(\"mps\")\n",
    "\n",
    "if mps_device is not None:\n",
    "    device = mps_device\n",
    "    print(\"Using MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 11.0526\n",
      "Epoch 1, Batch 200, Loss: 3.8290\n",
      "Epoch 1, Batch 300, Loss: 3.7077\n",
      "Epoch 1, Batch 400, Loss: 3.5304\n",
      "Epoch 1, Batch 500, Loss: 3.4397\n",
      "Epoch 1, Batch 600, Loss: 3.4255\n",
      "Epoch 1, Batch 700, Loss: 3.3803\n",
      "Epoch 1, Batch 800, Loss: 3.4102\n",
      "Epoch 1, Batch 900, Loss: 3.2100\n",
      "Epoch 1, Batch 1000, Loss: 3.2660\n",
      "Epoch 1, Batch 1100, Loss: 3.2784\n",
      "Epoch 1, Batch 1200, Loss: 3.1944\n",
      "Epoch 1, Batch 1300, Loss: 3.2385\n",
      "Epoch 1, Batch 1400, Loss: 3.0927\n",
      "Epoch 1, Batch 1500, Loss: 3.1436\n",
      "Epoch 1, Batch 1600, Loss: 3.0164\n",
      "Epoch 1, Batch 1700, Loss: 2.9513\n",
      "Epoch 1, Batch 1800, Loss: 2.9156\n",
      "Epoch 1, Batch 1900, Loss: 3.0340\n",
      "Epoch 1, Batch 2000, Loss: 2.9351\n",
      "Epoch 1, Batch 2100, Loss: 3.0008\n",
      "Epoch 1, Batch 2200, Loss: 2.7966\n",
      "Epoch 1, Batch 2300, Loss: 2.9193\n",
      "Epoch 1, Batch 2400, Loss: 2.9105\n",
      "Epoch 1, Batch 2500, Loss: 2.7258\n",
      "Epoch 1, Batch 2600, Loss: 2.9042\n",
      "Epoch 1, Batch 2700, Loss: 2.7513\n",
      "Epoch 1, Batch 2800, Loss: 2.7464\n",
      "Epoch 1, Batch 2900, Loss: 2.7426\n",
      "Epoch 2, Batch 100, Loss: 2.6449\n",
      "Epoch 2, Batch 200, Loss: 2.6947\n",
      "Epoch 2, Batch 300, Loss: 2.6569\n",
      "Epoch 2, Batch 400, Loss: 2.6337\n",
      "Epoch 2, Batch 500, Loss: 2.7617\n",
      "Epoch 2, Batch 600, Loss: 2.6988\n",
      "Epoch 2, Batch 700, Loss: 2.6201\n",
      "Epoch 2, Batch 800, Loss: 2.5366\n",
      "Epoch 2, Batch 900, Loss: 2.5772\n",
      "Epoch 2, Batch 1000, Loss: 2.5321\n",
      "Epoch 2, Batch 1100, Loss: 2.6096\n",
      "Epoch 2, Batch 1200, Loss: 2.5214\n",
      "Epoch 2, Batch 1300, Loss: 2.5252\n",
      "Epoch 2, Batch 1400, Loss: 2.4725\n",
      "Epoch 2, Batch 1500, Loss: 2.5120\n",
      "Epoch 2, Batch 1600, Loss: 2.4891\n",
      "Epoch 2, Batch 1700, Loss: 2.5116\n",
      "Epoch 2, Batch 1800, Loss: 2.4819\n",
      "Epoch 2, Batch 1900, Loss: 2.4956\n",
      "Epoch 2, Batch 2000, Loss: 2.4466\n",
      "Epoch 2, Batch 2100, Loss: 2.4654\n",
      "Epoch 2, Batch 2200, Loss: 2.4230\n",
      "Epoch 2, Batch 2300, Loss: 2.3998\n",
      "Epoch 2, Batch 2400, Loss: 2.4031\n",
      "Epoch 2, Batch 2500, Loss: 2.4313\n",
      "Epoch 2, Batch 2600, Loss: 2.3024\n",
      "Epoch 2, Batch 2700, Loss: 2.4096\n",
      "Epoch 2, Batch 2800, Loss: 2.3036\n",
      "Epoch 2, Batch 2900, Loss: 2.3728\n",
      "Epoch 3, Batch 100, Loss: 2.2850\n",
      "Epoch 3, Batch 200, Loss: 2.2905\n",
      "Epoch 3, Batch 300, Loss: 2.3371\n",
      "Epoch 3, Batch 400, Loss: 2.2562\n",
      "Epoch 3, Batch 500, Loss: 2.2961\n",
      "Epoch 3, Batch 600, Loss: 2.3287\n",
      "Epoch 3, Batch 700, Loss: 2.2818\n",
      "Epoch 3, Batch 800, Loss: 2.2075\n",
      "Epoch 3, Batch 900, Loss: 2.2402\n",
      "Epoch 3, Batch 1000, Loss: 2.2373\n",
      "Epoch 3, Batch 1100, Loss: 2.2124\n",
      "Epoch 3, Batch 1200, Loss: 2.2443\n",
      "Epoch 3, Batch 1300, Loss: 2.1430\n",
      "Epoch 3, Batch 1400, Loss: 2.2226\n",
      "Epoch 3, Batch 1500, Loss: 2.1645\n",
      "Epoch 3, Batch 1600, Loss: 2.1688\n",
      "Epoch 3, Batch 1700, Loss: 2.2334\n",
      "Epoch 3, Batch 1800, Loss: 2.2073\n",
      "Epoch 3, Batch 1900, Loss: 2.1784\n",
      "Epoch 3, Batch 2000, Loss: 2.1287\n",
      "Epoch 3, Batch 2100, Loss: 2.1125\n",
      "Epoch 3, Batch 2200, Loss: 2.1717\n",
      "Epoch 3, Batch 2300, Loss: 2.0771\n",
      "Epoch 3, Batch 2400, Loss: 2.2417\n",
      "Epoch 3, Batch 2500, Loss: 2.1496\n",
      "Epoch 3, Batch 2600, Loss: 2.1059\n",
      "Epoch 3, Batch 2700, Loss: 2.0985\n",
      "Epoch 3, Batch 2800, Loss: 2.1374\n",
      "Epoch 3, Batch 2900, Loss: 2.1234\n",
      "Epoch 4, Batch 100, Loss: 2.0896\n",
      "Epoch 4, Batch 200, Loss: 1.9870\n",
      "Epoch 4, Batch 300, Loss: 2.0143\n",
      "Epoch 4, Batch 400, Loss: 2.0528\n",
      "Epoch 4, Batch 500, Loss: 2.0079\n",
      "Epoch 4, Batch 600, Loss: 1.9720\n",
      "Epoch 4, Batch 700, Loss: 1.9921\n",
      "Epoch 4, Batch 800, Loss: 2.0253\n",
      "Epoch 4, Batch 900, Loss: 2.0059\n",
      "Epoch 4, Batch 1000, Loss: 1.9702\n",
      "Epoch 4, Batch 1100, Loss: 2.0467\n",
      "Epoch 4, Batch 1200, Loss: 1.9688\n",
      "Epoch 4, Batch 1300, Loss: 2.0154\n",
      "Epoch 4, Batch 1400, Loss: 1.9160\n",
      "Epoch 4, Batch 1500, Loss: 1.9915\n",
      "Epoch 4, Batch 1600, Loss: 1.9523\n",
      "Epoch 4, Batch 1700, Loss: 1.9565\n",
      "Epoch 4, Batch 1800, Loss: 1.9816\n",
      "Epoch 4, Batch 1900, Loss: 1.9400\n",
      "Epoch 4, Batch 2000, Loss: 1.9602\n",
      "Epoch 4, Batch 2100, Loss: 1.9470\n",
      "Epoch 4, Batch 2200, Loss: 1.8753\n",
      "Epoch 4, Batch 2300, Loss: 1.9496\n",
      "Epoch 4, Batch 2400, Loss: 2.0421\n",
      "Epoch 4, Batch 2500, Loss: 1.9109\n",
      "Epoch 4, Batch 2600, Loss: 1.8979\n",
      "Epoch 4, Batch 2700, Loss: 1.9741\n",
      "Epoch 4, Batch 2800, Loss: 1.9928\n",
      "Epoch 4, Batch 2900, Loss: 1.9416\n",
      "Epoch 5, Batch 100, Loss: 1.8087\n",
      "Epoch 5, Batch 200, Loss: 1.8305\n",
      "Epoch 5, Batch 300, Loss: 1.7987\n",
      "Epoch 5, Batch 400, Loss: 1.8039\n",
      "Epoch 5, Batch 500, Loss: 1.8056\n",
      "Epoch 5, Batch 600, Loss: 1.7703\n",
      "Epoch 5, Batch 700, Loss: 1.7447\n",
      "Epoch 5, Batch 800, Loss: 1.8325\n",
      "Epoch 5, Batch 900, Loss: 1.7372\n",
      "Epoch 5, Batch 1000, Loss: 1.7470\n",
      "Epoch 5, Batch 1100, Loss: 1.7943\n",
      "Epoch 5, Batch 1200, Loss: 1.7911\n",
      "Epoch 5, Batch 1300, Loss: 1.7878\n",
      "Epoch 5, Batch 1400, Loss: 1.7336\n",
      "Epoch 5, Batch 1500, Loss: 1.7790\n",
      "Epoch 5, Batch 1600, Loss: 1.7378\n",
      "Epoch 5, Batch 1700, Loss: 1.6972\n",
      "Epoch 5, Batch 1800, Loss: 1.7285\n",
      "Epoch 5, Batch 1900, Loss: 1.7770\n",
      "Epoch 5, Batch 2000, Loss: 1.7585\n",
      "Epoch 5, Batch 2100, Loss: 1.7740\n",
      "Epoch 5, Batch 2200, Loss: 1.8037\n",
      "Epoch 5, Batch 2300, Loss: 1.7247\n",
      "Epoch 5, Batch 2400, Loss: 1.7160\n",
      "Epoch 5, Batch 2500, Loss: 1.7712\n",
      "Epoch 5, Batch 2600, Loss: 1.6895\n",
      "Epoch 5, Batch 2700, Loss: 1.7080\n",
      "Epoch 5, Batch 2800, Loss: 1.7200\n",
      "Epoch 5, Batch 2900, Loss: 1.7858\n",
      "CPU times: user 55.2 s, sys: 7.72 s, total: 1min 2s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define the dataset class\n",
    "class RatingDataset(Dataset):\n",
    "    \"\"\"Dataset for loading user-item ratings for training\"\"\"\n",
    "    def __init__(self, user_ids, item_ids, ratings):\n",
    "        self.user_ids = torch.tensor(user_ids, dtype=torch.int64)\n",
    "        self.item_ids = torch.tensor(item_ids, dtype=torch.int64)\n",
    "        self.ratings = torch.tensor(ratings, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.item_ids[idx], self.ratings[idx]\n",
    "\n",
    "class WideAndDeep(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=20, hidden_layers=[64, 32, 16], dropout_rate=0.2):\n",
    "        super(WideAndDeep, self).__init__()\n",
    "        # Wide part\n",
    "        self.wide = nn.Embedding(num_users + num_items, 1)\n",
    "        # Deep part\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        input_size = 2 * embedding_dim  # Concatenated user and item embeddings\n",
    "        for hidden_layer in hidden_layers:\n",
    "            self.fc_layers.append(nn.Linear(input_size, hidden_layer))\n",
    "            input_size = hidden_layer\n",
    "        self.fc_layers.append(nn.Linear(input_size, 1))  # Final output layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        # Wide part\n",
    "        wide_out = self.wide(user_indices + item_indices)\n",
    "        # Deep part\n",
    "        user_embedding = self.user_embedding(user_indices)\n",
    "        item_embedding = self.item_embedding(item_indices)\n",
    "        x = torch.cat([user_embedding, item_embedding], dim=-1)\n",
    "        for layer in self.fc_layers[:-1]:\n",
    "            x = self.relu(layer(x))\n",
    "            x = self.dropout(x)\n",
    "        deep_out = self.fc_layers[-1](x)\n",
    "        # Combined part\n",
    "        combined_out = wide_out + deep_out\n",
    "        return combined_out.squeeze()\n",
    "\n",
    "# Training function\n",
    "def train_model(model, data_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (users, items, ratings) in enumerate(data_loader):\n",
    "            users = users.to(device)  # Move data to GPU\n",
    "            items = items.to(device)  # Move data to GPU\n",
    "            ratings = ratings.to(device)  # Move data to GPU\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(users, items)\n",
    "            loss = criterion(outputs, ratings)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if batch_idx % 100 == 99:\n",
    "                print(f'Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {running_loss / 100:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "# Prepare data\n",
    "train_data = pd.read_csv(\"cs608_ip_train_v3.csv\")\n",
    "train_data['user_id'] = train_data['user_id'].astype('category').cat.codes\n",
    "train_data['item_id'] = train_data['item_id'].astype('category').cat.codes\n",
    "dataset = RatingDataset(train_data['user_id'], train_data['item_id'], train_data['rating'])\n",
    "\n",
    "# Create DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "num_users = train_data['user_id'].nunique()\n",
    "num_items = train_data['item_id'].nunique()\n",
    "model = WideAndDeep(num_users, num_items).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, data_loader, criterion, optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"wide_and_deep.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.4555642481171522\n",
      "Accuracy: 0.3121782765877084\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# Load your CSV data\n",
    "data_path = \"./cs608_ip_probe_v3.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Assume df has columns 'user_id', 'item_id', which we need to convert to tensor\n",
    "# Also assume that 'ratings' column is your target\n",
    "users = torch.tensor(df[\"user_id\"].values).to(device)\n",
    "items = torch.tensor(df[\"item_id\"].values).to(device)\n",
    "ratings = torch.tensor(df[\"rating\"].values)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Create a data loader for batch processing\n",
    "dataset = TensorDataset(users, items, ratings)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# To store predictions and actual values\n",
    "predictions, actuals = [], []\n",
    "\n",
    "# Evaluate the model\n",
    "for user, item, rating in data_loader:\n",
    "    with torch.no_grad():\n",
    "        output = model(user, item)\n",
    "        predictions.extend(output.cpu().numpy())\n",
    "        actuals.extend(rating.cpu().numpy())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "predictions = np.array(predictions)\n",
    "actuals = np.array(actuals)\n",
    "rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "accuracy = accuracy_score(actuals, predictions.round())\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def generate_recommendations(model, num_users, num_items, top_k=50):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    recommendations = []\n",
    "\n",
    "    # Iterate over all users\n",
    "    for user_id in tqdm(range(num_users)):\n",
    "        user_tensor = torch.tensor(\n",
    "            [user_id] * num_items, dtype=torch.int64\n",
    "        ).to(device)  # Repeat user ID for each item\n",
    "        item_tensor = torch.tensor(range(num_items), dtype=torch.int64).to(device)  # All item IDs\n",
    "\n",
    "        # Predict scores for all items for this user\n",
    "        with torch.no_grad():\n",
    "            scores = (\n",
    "                model(user_tensor, item_tensor).cpu().numpy()\n",
    "            )  # Get scores and move to CPU\n",
    "\n",
    "        # Get the indices of the top k scores\n",
    "        top_item_indices = scores.argsort()[-top_k:][\n",
    "            ::-1\n",
    "        ]  # Indices of top scoring items\n",
    "\n",
    "        # Append to the list of recommendations\n",
    "        recommendations.append(top_item_indices.tolist())\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da2317fece54043b7874c8ddbb17161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 7.34 s, total: 1min 37s\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# Number of users and items\n",
    "num_users = train_data[\"user_id\"].nunique()\n",
    "num_items = train_data[\"item_id\"].nunique()\n",
    "\n",
    "# Generate recommendations for all users\n",
    "top_k_recommendations = generate_recommendations(model, num_users, num_items)\n",
    "\n",
    "with open(\"submission.txt\", \"w\") as file:\n",
    "    for user_recommendations in top_k_recommendations:\n",
    "        file.write(\" \".join(map(str, user_recommendations)) + \"\\n\")\n",
    "\n",
    "# zip the submission file\n",
    "with zipfile.ZipFile('submission.zip', 'w') as file:\n",
    "    file.write('submission.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
